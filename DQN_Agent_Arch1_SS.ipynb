{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check what the max, min and mean time values are. This will help us in defining the 'next_step' function in the Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "11.0\n",
      "0.0\n",
      "3.0542857142857143\n",
      "7.93705306122449\n"
     ]
    }
   ],
   "source": [
    "print(type(Time_matrix))\n",
    "print(Time_matrix.max())\n",
    "print(Time_matrix.min())\n",
    "print(Time_matrix.mean())\n",
    "print(Time_matrix.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the max time is 11 hours between any 2 points, the next state of the cab driver may increase at most by  1 day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.06 # 0.06 after fix was better\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_max = 1\n",
    "        self.epsilon_decay = -0.0007 #for 3k\n",
    "        self.epsilon_min = 0.00001\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # Initialize the value of the states tracked\n",
    "        self.states_tracked = []\n",
    "        \n",
    "        # We are going to track state [0,0,0] and action () at index 2 in the action space.\n",
    "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Function that takes in the agent and constructs the network\n",
    "        to train it\n",
    "        @return model\n",
    "        @params agent\n",
    "        \"\"\"\n",
    "        input_shape = self.state_size\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, possible_actions_index, actions):\n",
    "        \"\"\"\n",
    "        get action in a state according to an epsilon-greedy approach\n",
    "        possible_actions_index, actions are the 'ride requests' that teh driver got.\n",
    "        \"\"\"        \n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after each episode       \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # explore: choose a random action from the ride requests\n",
    "            return random.choice(possible_actions_index)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "\n",
    "            # Use the model to predict the Q_values.\n",
    "            q_value = self.model.predict(state)\n",
    "\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
    "\n",
    "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, done):\n",
    "        \"\"\"appends the new agent run output to replay buffer\"\"\"\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "        \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        \"\"\" \n",
    "        Function to train the model on eacg step run.\n",
    "        Picks the random memory events according to batch size and \n",
    "        runs it through the network to train it.\n",
    "        \"\"\"\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                update_input[i] = env.state_encod_arch1(state)     \n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            target = self.model.predict(update_input)\n",
    "            # target for q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "            # model fit\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "    def save_tracking_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_tracked.append(q_value[0][2])\n",
    "\n",
    "    def save(self, name):\n",
    "        with open(name, 'wb') as file:  \n",
    "            pickle.dump(self.model, file,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    while !terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        # 2. Evaluate your reward and next state\n",
    "        # 3. Append the experience to the memory\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_time = 24*30 #30 days before which car has to be recharged\n",
    "n_episodes = 3000\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "\n",
    "# Invoke Env class\n",
    "env = CabDriver()\n",
    "action_space, state_space, state = env.reset()\n",
    "\n",
    "# Set up state and action sizes.\n",
    "state_size = m+t+d\n",
    "action_size = len(action_space)\n",
    "\n",
    "# Invoke agent class\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "# Rewards for state [0,0,0] being tracked.\n",
    "rewards_init_state = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the episodes build up replay buffer and train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward -130.0, memory_length 122, epsilon 0.99999 total_time 721.0\n",
      "Saving Model 0\n",
      "episode 10, reward -120.0, memory_length 1505, epsilon 0.9930145126888058 total_time 725.0\n",
      "episode 20, reward -318.0, memory_length 2000, epsilon 0.9860876832874194 total_time 723.0\n",
      "episode 30, reward -160.0, memory_length 2000, epsilon 0.9792091723798139 total_time 730.0\n",
      "episode 40, reward 27.0, memory_length 2000, epsilon 0.9723786429175789 total_time 723.0\n",
      "episode 50, reward -323.0, memory_length 2000, epsilon 0.965595760203404 total_time 728.0\n",
      "episode 60, reward -123.0, memory_length 2000, epsilon 0.9588601918746789 total_time 721.0\n",
      "episode 70, reward -43.0, memory_length 2000, epsilon 0.9521716078872079 total_time 723.0\n",
      "episode 80, reward -246.0, memory_length 2000, epsilon 0.9455296804990374 total_time 727.0\n",
      "episode 90, reward -213.0, memory_length 2000, epsilon 0.9389340842543964 total_time 721.0\n",
      "episode 100, reward -354.0, memory_length 2000, epsilon 0.9323844959677493 total_time 721.0\n",
      "episode 110, reward 48.0, memory_length 2000, epsilon 0.9258805947079594 total_time 721.0\n",
      "episode 120, reward 10.0, memory_length 2000, epsilon 0.9194220617825638 total_time 728.0\n",
      "episode 130, reward -93.0, memory_length 2000, epsilon 0.9130085807221568 total_time 721.0\n",
      "episode 140, reward -9.0, memory_length 2000, epsilon 0.9066398372648834 total_time 730.0\n",
      "episode 150, reward -116.0, memory_length 2000, epsilon 0.9003155193410398 total_time 725.0\n",
      "episode 160, reward 117.0, memory_length 2000, epsilon 0.8940353170577823 total_time 729.0\n",
      "episode 170, reward -29.0, memory_length 2000, epsilon 0.8877989226839426 total_time 721.0\n",
      "episode 180, reward -135.0, memory_length 2000, epsilon 0.8816060306349482 total_time 723.0\n",
      "episode 190, reward -52.0, memory_length 2000, epsilon 0.87545633745785 total_time 724.0\n",
      "episode 200, reward -315.0, memory_length 2000, epsilon 0.8693495418164519 total_time 726.0\n",
      "episode 210, reward -59.0, memory_length 2000, epsilon 0.8632853444765453 total_time 728.0\n",
      "episode 220, reward -170.0, memory_length 2000, epsilon 0.8572634482912475 total_time 727.0\n",
      "episode 230, reward -287.0, memory_length 2000, epsilon 0.8512835581864401 total_time 726.0\n",
      "episode 240, reward 19.0, memory_length 2000, epsilon 0.845345381146312 total_time 725.0\n",
      "episode 250, reward -488.0, memory_length 2000, epsilon 0.8394486261989997 total_time 723.0\n",
      "episode 260, reward -70.0, memory_length 2000, epsilon 0.8335930044023312 total_time 725.0\n",
      "episode 270, reward -13.0, memory_length 2000, epsilon 0.8277782288296667 total_time 725.0\n",
      "episode 280, reward 132.0, memory_length 2000, epsilon 0.8220040145558398 total_time 722.0\n",
      "episode 290, reward 33.0, memory_length 2000, epsilon 0.8162700786431957 total_time 721.0\n",
      "episode 300, reward -70.0, memory_length 2000, epsilon 0.8105761401277274 total_time 725.0\n",
      "episode 310, reward -274.0, memory_length 2000, epsilon 0.8049219200053085 total_time 727.0\n",
      "episode 320, reward -27.0, memory_length 2000, epsilon 0.7993071412180214 total_time 729.0\n",
      "episode 330, reward 38.0, memory_length 2000, epsilon 0.7937315286405824 total_time 725.0\n",
      "episode 340, reward -129.0, memory_length 2000, epsilon 0.7881948090668595 total_time 721.0\n",
      "episode 350, reward 135.0, memory_length 2000, epsilon 0.7826967111964858 total_time 729.0\n",
      "episode 360, reward -74.0, memory_length 2000, epsilon 0.7772369656215654 total_time 723.0\n",
      "episode 370, reward 225.0, memory_length 2000, epsilon 0.771815304813473 total_time 721.0\n",
      "episode 380, reward 88.0, memory_length 2000, epsilon 0.7664314631097442 total_time 724.0\n",
      "episode 390, reward 194.0, memory_length 2000, epsilon 0.761085176701058 total_time 727.0\n",
      "episode 400, reward -250.0, memory_length 2000, epsilon 0.7557761836183109 total_time 722.0\n",
      "episode 410, reward -198.0, memory_length 2000, epsilon 0.7505042237197797 total_time 726.0\n",
      "episode 420, reward 72.0, memory_length 2000, epsilon 0.7452690386783742 total_time 727.0\n",
      "episode 430, reward 212.0, memory_length 2000, epsilon 0.7400703719689802 total_time 732.0\n",
      "episode 440, reward -505.0, memory_length 2000, epsilon 0.7349079688558887 total_time 721.0\n",
      "episode 450, reward -49.0, memory_length 2000, epsilon 0.7297815763803142 total_time 724.0\n",
      "episode 460, reward 355.0, memory_length 2000, epsilon 0.7246909433479997 total_time 725.0\n",
      "episode 470, reward -40.0, memory_length 2000, epsilon 0.719635820316908 total_time 726.0\n",
      "episode 480, reward 188.0, memory_length 2000, epsilon 0.7146159595849991 total_time 726.0\n",
      "episode 490, reward -8.0, memory_length 2000, epsilon 0.7096311151780931 total_time 724.0\n",
      "episode 500, reward 280.0, memory_length 2000, epsilon 0.7046810428378163 total_time 721.0\n",
      "Saving Model 500\n",
      "episode 510, reward 292.0, memory_length 2000, epsilon 0.6997655000096337 total_time 726.0\n",
      "episode 520, reward 81.0, memory_length 2000, epsilon 0.6948842458309632 total_time 726.0\n",
      "episode 530, reward 392.0, memory_length 2000, epsilon 0.6900370411193735 total_time 727.0\n",
      "episode 540, reward 356.0, memory_length 2000, epsilon 0.6852236483608637 total_time 725.0\n",
      "episode 550, reward 368.0, memory_length 2000, epsilon 0.6804438316982256 total_time 727.0\n",
      "episode 560, reward -29.0, memory_length 2000, epsilon 0.6756973569194864 total_time 726.0\n",
      "episode 570, reward -67.0, memory_length 2000, epsilon 0.6709839914464324 total_time 730.0\n",
      "episode 580, reward -137.0, memory_length 2000, epsilon 0.6663035043232122 total_time 722.0\n",
      "episode 590, reward 16.0, memory_length 2000, epsilon 0.66165566620502 total_time 732.0\n",
      "episode 600, reward 145.0, memory_length 2000, epsilon 0.6570402493468587 total_time 729.0\n",
      "episode 610, reward 36.0, memory_length 2000, epsilon 0.6524570275923782 total_time 721.0\n",
      "episode 620, reward -301.0, memory_length 2000, epsilon 0.6479057763627958 total_time 728.0\n",
      "episode 630, reward -260.0, memory_length 2000, epsilon 0.6433862726458905 total_time 727.0\n",
      "episode 640, reward 432.0, memory_length 2000, epsilon 0.638898294985076 total_time 721.0\n",
      "episode 650, reward -35.0, memory_length 2000, epsilon 0.6344416234685488 total_time 726.0\n",
      "episode 660, reward 75.0, memory_length 2000, epsilon 0.6300160397185128 total_time 728.0\n",
      "episode 670, reward 146.0, memory_length 2000, epsilon 0.6256213268804792 total_time 727.0\n",
      "episode 680, reward 133.0, memory_length 2000, epsilon 0.6212572696126393 total_time 729.0\n",
      "episode 690, reward 128.0, memory_length 2000, epsilon 0.6169236540753137 total_time 737.0\n",
      "episode 700, reward 159.0, memory_length 2000, epsilon 0.6126202679204743 total_time 724.0\n",
      "episode 710, reward 171.0, memory_length 2000, epsilon 0.6083469002813382 total_time 725.0\n",
      "episode 720, reward 27.0, memory_length 2000, epsilon 0.6041033417620362 total_time 724.0\n",
      "episode 730, reward 95.0, memory_length 2000, epsilon 0.5998893844273517 total_time 724.0\n",
      "episode 740, reward -126.0, memory_length 2000, epsilon 0.5957048217925323 total_time 731.0\n",
      "episode 750, reward 106.0, memory_length 2000, epsilon 0.5915494488131715 total_time 727.0\n",
      "episode 760, reward -132.0, memory_length 2000, epsilon 0.5874230618751618 total_time 726.0\n",
      "episode 770, reward 308.0, memory_length 2000, epsilon 0.5833254587847179 total_time 727.0\n",
      "episode 780, reward 266.0, memory_length 2000, epsilon 0.5792564387584682 total_time 722.0\n",
      "episode 790, reward 185.0, memory_length 2000, epsilon 0.5752158024136176 total_time 723.0\n",
      "episode 800, reward 379.0, memory_length 2000, epsilon 0.5712033517581764 total_time 731.0\n",
      "episode 810, reward 320.0, memory_length 2000, epsilon 0.5672188901812599 total_time 729.0\n",
      "episode 820, reward -164.0, memory_length 2000, epsilon 0.5632622224434535 total_time 722.0\n",
      "episode 830, reward 410.0, memory_length 2000, epsilon 0.5593331546672463 total_time 725.0\n",
      "episode 840, reward 90.0, memory_length 2000, epsilon 0.5554314943275312 total_time 723.0\n",
      "episode 850, reward 152.0, memory_length 2000, epsilon 0.5515570502421712 total_time 721.0\n",
      "episode 860, reward 232.0, memory_length 2000, epsilon 0.5477096325626304 total_time 721.0\n",
      "episode 870, reward 68.0, memory_length 2000, epsilon 0.5438890527646728 total_time 729.0\n",
      "episode 880, reward 378.0, memory_length 2000, epsilon 0.5400951236391243 total_time 722.0\n",
      "episode 890, reward -111.0, memory_length 2000, epsilon 0.536327659282698 total_time 721.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 900, reward 194.0, memory_length 2000, epsilon 0.5325864750888871 total_time 729.0\n",
      "episode 910, reward 240.0, memory_length 2000, epsilon 0.5288713877389174 total_time 725.0\n",
      "episode 920, reward -162.0, memory_length 2000, epsilon 0.5251822151927655 total_time 723.0\n",
      "episode 930, reward 109.0, memory_length 2000, epsilon 0.5215187766802384 total_time 728.0\n",
      "episode 940, reward 125.0, memory_length 2000, epsilon 0.5178808926921159 total_time 723.0\n",
      "episode 950, reward -155.0, memory_length 2000, epsilon 0.5142683849713549 total_time 721.0\n",
      "episode 960, reward 59.0, memory_length 2000, epsilon 0.5106810765043542 total_time 729.0\n",
      "episode 970, reward 397.0, memory_length 2000, epsilon 0.5071187915122811 total_time 728.0\n",
      "episode 980, reward 17.0, memory_length 2000, epsilon 0.5035813554424584 total_time 731.0\n",
      "episode 990, reward 122.0, memory_length 2000, epsilon 0.5000685949598108 total_time 727.0\n",
      "episode 1000, reward 360.0, memory_length 2000, epsilon 0.49658033793837164 total_time 724.0\n",
      "Saving Model 1000\n",
      "episode 1010, reward 96.0, memory_length 2000, epsilon 0.4931164134528491 total_time 726.0\n",
      "episode 1020, reward 322.0, memory_length 2000, epsilon 0.4896766517702504 total_time 721.0\n",
      "episode 1030, reward 132.0, memory_length 2000, epsilon 0.4862608843415646 total_time 732.0\n",
      "episode 1040, reward 129.0, memory_length 2000, epsilon 0.4828689437935045 total_time 725.0\n",
      "episode 1050, reward 370.0, memory_length 2000, epsilon 0.4795006639203044 total_time 723.0\n",
      "episode 1060, reward 581.0, memory_length 2000, epsilon 0.47615587967557665 total_time 721.0\n",
      "episode 1070, reward 36.0, memory_length 2000, epsilon 0.472834427164224 total_time 721.0\n",
      "episode 1080, reward 180.0, memory_length 2000, epsilon 0.4695361436344088 total_time 729.0\n",
      "episode 1090, reward -11.0, memory_length 2000, epsilon 0.4662608674695783 total_time 721.0\n",
      "episode 1100, reward 227.0, memory_length 2000, epsilon 0.463008438180545 total_time 722.0\n",
      "episode 1110, reward 406.0, memory_length 2000, epsilon 0.4597786963976229 total_time 723.0\n",
      "episode 1120, reward 16.0, memory_length 2000, epsilon 0.4565714838628185 total_time 721.0\n",
      "episode 1130, reward 109.0, memory_length 2000, epsilon 0.4533866434220759 total_time 727.0\n",
      "episode 1140, reward 155.0, memory_length 2000, epsilon 0.45022401901757625 total_time 721.0\n",
      "episode 1150, reward 577.0, memory_length 2000, epsilon 0.4470834556800909 total_time 726.0\n",
      "episode 1160, reward 275.0, memory_length 2000, epsilon 0.44396479952138795 total_time 732.0\n",
      "episode 1170, reward -4.0, memory_length 2000, epsilon 0.4408678977266917 total_time 727.0\n",
      "episode 1180, reward 595.0, memory_length 2000, epsilon 0.4377925985471945 total_time 722.0\n",
      "episode 1190, reward 90.0, memory_length 2000, epsilon 0.43473875129262124 total_time 724.0\n",
      "episode 1200, reward 126.0, memory_length 2000, epsilon 0.43170620632384543 total_time 721.0\n",
      "episode 1210, reward 354.0, memory_length 2000, epsilon 0.4286948150455569 total_time 722.0\n",
      "episode 1220, reward 246.0, memory_length 2000, epsilon 0.42570442989898033 total_time 721.0\n",
      "episode 1230, reward 81.0, memory_length 2000, epsilon 0.4227349043546454 total_time 731.0\n",
      "episode 1240, reward 284.0, memory_length 2000, epsilon 0.4197860929052062 total_time 726.0\n",
      "episode 1250, reward 6.0, memory_length 2000, epsilon 0.41685785105831163 total_time 723.0\n",
      "episode 1260, reward -53.0, memory_length 2000, epsilon 0.4139500353295254 total_time 721.0\n",
      "episode 1270, reward 225.0, memory_length 2000, epsilon 0.411062503235295 total_time 723.0\n",
      "episode 1280, reward 79.0, memory_length 2000, epsilon 0.4081951132859699 total_time 722.0\n",
      "episode 1290, reward -119.0, memory_length 2000, epsilon 0.40534772497886906 total_time 724.0\n",
      "episode 1300, reward 182.0, memory_length 2000, epsilon 0.40252019879139567 total_time 722.0\n",
      "episode 1310, reward -4.0, memory_length 2000, epsilon 0.39971239617420073 total_time 728.0\n",
      "episode 1320, reward 376.0, memory_length 2000, epsilon 0.39692417954439424 total_time 722.0\n",
      "episode 1330, reward 77.0, memory_length 2000, epsilon 0.3941554122788035 total_time 727.0\n",
      "episode 1340, reward 232.0, memory_length 2000, epsilon 0.3914059587072785 total_time 724.0\n",
      "episode 1350, reward 226.0, memory_length 2000, epsilon 0.38867568410604403 total_time 721.0\n",
      "episode 1360, reward -36.0, memory_length 2000, epsilon 0.38596445469109847 total_time 721.0\n",
      "episode 1370, reward 318.0, memory_length 2000, epsilon 0.3832721376116579 total_time 723.0\n",
      "episode 1380, reward 212.0, memory_length 2000, epsilon 0.3805986009436468 total_time 727.0\n",
      "episode 1390, reward 498.0, memory_length 2000, epsilon 0.3779437136832335 total_time 727.0\n",
      "episode 1400, reward 104.0, memory_length 2000, epsilon 0.3753073457404111 total_time 721.0\n",
      "episode 1410, reward 340.0, memory_length 2000, epsilon 0.37268936793262275 total_time 721.0\n",
      "episode 1420, reward 396.0, memory_length 2000, epsilon 0.3700896519784322 total_time 726.0\n",
      "episode 1430, reward 296.0, memory_length 2000, epsilon 0.3675080704912375 total_time 724.0\n",
      "episode 1440, reward 230.0, memory_length 2000, epsilon 0.3649444969730292 total_time 728.0\n",
      "episode 1450, reward 248.0, memory_length 2000, epsilon 0.36239880580819206 total_time 729.0\n",
      "episode 1460, reward 312.0, memory_length 2000, epsilon 0.35987087225734954 total_time 727.0\n",
      "episode 1470, reward 249.0, memory_length 2000, epsilon 0.35736057245125197 total_time 725.0\n",
      "episode 1480, reward 173.0, memory_length 2000, epsilon 0.3548677833847064 total_time 730.0\n",
      "episode 1490, reward 45.0, memory_length 2000, epsilon 0.3523923829105501 total_time 732.0\n",
      "episode 1500, reward 284.0, memory_length 2000, epsilon 0.3499342497336642 total_time 725.0\n",
      "Saving Model 1500\n",
      "episode 1510, reward 81.0, memory_length 2000, epsilon 0.3474932634050315 total_time 726.0\n",
      "episode 1520, reward -88.0, memory_length 2000, epsilon 0.3450693043158333 total_time 725.0\n",
      "episode 1530, reward 106.0, memory_length 2000, epsilon 0.3426622536915894 total_time 730.0\n",
      "episode 1540, reward -96.0, memory_length 2000, epsilon 0.3402719935863374 total_time 721.0\n",
      "episode 1550, reward 123.0, memory_length 2000, epsilon 0.33789840687685413 total_time 727.0\n",
      "episode 1560, reward 413.0, memory_length 2000, epsilon 0.33554137725691563 total_time 726.0\n",
      "episode 1570, reward 289.0, memory_length 2000, epsilon 0.3332007892315991 total_time 721.0\n",
      "episode 1580, reward 130.0, memory_length 2000, epsilon 0.330876528111623 total_time 725.0\n",
      "episode 1590, reward 556.0, memory_length 2000, epsilon 0.32856848000772737 total_time 721.0\n",
      "episode 1600, reward 482.0, memory_length 2000, epsilon 0.3262765318250933 total_time 725.0\n",
      "episode 1610, reward 302.0, memory_length 2000, epsilon 0.32400057125780124 total_time 725.0\n",
      "episode 1620, reward 41.0, memory_length 2000, epsilon 0.321740486783328 total_time 726.0\n",
      "episode 1630, reward 72.0, memory_length 2000, epsilon 0.31949616765708216 total_time 722.0\n",
      "episode 1640, reward 119.0, memory_length 2000, epsilon 0.3172675039069775 total_time 726.0\n",
      "episode 1650, reward 158.0, memory_length 2000, epsilon 0.3150543863280443 total_time 729.0\n",
      "episode 1660, reward 339.0, memory_length 2000, epsilon 0.3128567064770785 total_time 724.0\n",
      "episode 1670, reward 339.0, memory_length 2000, epsilon 0.3106743566673275 total_time 723.0\n",
      "episode 1680, reward 72.0, memory_length 2000, epsilon 0.30850722996321406 total_time 721.0\n",
      "episode 1690, reward 244.0, memory_length 2000, epsilon 0.3063552201750961 total_time 723.0\n",
      "episode 1700, reward 173.0, memory_length 2000, epsilon 0.30421822185406344 total_time 724.0\n",
      "episode 1710, reward -99.0, memory_length 2000, epsilon 0.3020961302867706 total_time 722.0\n",
      "episode 1720, reward 334.0, memory_length 2000, epsilon 0.2999888414903064 total_time 724.0\n",
      "episode 1730, reward 264.0, memory_length 2000, epsilon 0.2978962522070981 total_time 721.0\n",
      "episode 1740, reward 416.0, memory_length 2000, epsilon 0.2958182598998521 total_time 724.0\n",
      "episode 1750, reward 199.0, memory_length 2000, epsilon 0.29375476274652956 total_time 723.0\n",
      "episode 1760, reward 452.0, memory_length 2000, epsilon 0.29170565963535716 total_time 736.0\n",
      "episode 1770, reward 356.0, memory_length 2000, epsilon 0.2896708501598725 total_time 731.0\n",
      "episode 1780, reward 271.0, memory_length 2000, epsilon 0.28765023461400396 total_time 722.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1790, reward 168.0, memory_length 2000, epsilon 0.2856437139871857 total_time 723.0\n",
      "episode 1800, reward 182.0, memory_length 2000, epsilon 0.2836511899595054 total_time 731.0\n",
      "episode 1810, reward 288.0, memory_length 2000, epsilon 0.28167256489688713 total_time 733.0\n",
      "episode 1820, reward 231.0, memory_length 2000, epsilon 0.2797077418463068 total_time 732.0\n",
      "episode 1830, reward 415.0, memory_length 2000, epsilon 0.277756624531042 total_time 724.0\n",
      "episode 1840, reward 339.0, memory_length 2000, epsilon 0.2758191173459537 total_time 724.0\n",
      "episode 1850, reward 394.0, memory_length 2000, epsilon 0.2738951253528023 total_time 724.0\n",
      "episode 1860, reward 177.0, memory_length 2000, epsilon 0.27198455427559504 total_time 731.0\n",
      "episode 1870, reward 72.0, memory_length 2000, epsilon 0.27008731049596707 total_time 725.0\n",
      "episode 1880, reward 208.0, memory_length 2000, epsilon 0.26820330104859336 total_time 731.0\n",
      "episode 1890, reward 137.0, memory_length 2000, epsilon 0.2663324336166342 total_time 723.0\n",
      "episode 1900, reward 364.0, memory_length 2000, epsilon 0.264474616527211 total_time 723.0\n",
      "episode 1910, reward 139.0, memory_length 2000, epsilon 0.2626297587469147 total_time 723.0\n",
      "episode 1920, reward 387.0, memory_length 2000, epsilon 0.2607977698773448 total_time 728.0\n",
      "episode 1930, reward -9.0, memory_length 2000, epsilon 0.25897856015068044 total_time 728.0\n",
      "episode 1940, reward 203.0, memory_length 2000, epsilon 0.2571720404252807 total_time 733.0\n",
      "episode 1950, reward 537.0, memory_length 2000, epsilon 0.25537812218131783 total_time 721.0\n",
      "episode 1960, reward 83.0, memory_length 2000, epsilon 0.2535967175164388 total_time 722.0\n",
      "episode 1970, reward 259.0, memory_length 2000, epsilon 0.2518277391414586 total_time 729.0\n",
      "episode 1980, reward 165.0, memory_length 2000, epsilon 0.25007110037608304 total_time 729.0\n",
      "episode 1990, reward 223.0, memory_length 2000, epsilon 0.24832671514466093 total_time 721.0\n",
      "episode 2000, reward 259.0, memory_length 2000, epsilon 0.24659449797196709 total_time 728.0\n",
      "Saving Model 2000\n",
      "episode 2010, reward 206.0, memory_length 2000, epsilon 0.24487436397901335 total_time 723.0\n",
      "episode 2020, reward 167.0, memory_length 2000, epsilon 0.24316622887889003 total_time 730.0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "score_tracked = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    track_reward = False\n",
    "    # reset at the start of each episode\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n",
    "    initial_state = env.state_init\n",
    "    if (initial_state == [0,0,0]):\n",
    "        track_reward = True\n",
    "\n",
    "    total_time = 0  # Total time driver rode in this episode\n",
    "    while not done:\n",
    "        # 1. Get a list of the ride requests driver got.\n",
    "        possible_actions_indices, actions = env.requests(state)\n",
    "        # 2. Pick epsilon-greedy action from possible actions for the current state.\n",
    "        action = agent.get_action(state, possible_actions_indices, actions)\n",
    "\n",
    "        # 3. Evaluate your reward and next state\n",
    "        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
    "\n",
    "        # 4. Total time driver rode in this episode\n",
    "        total_time += step_time\n",
    "        if (total_time > episode_time):\n",
    "            # if ride does not complete in stipu;ated time skip\n",
    "            # it and move to next episode.\n",
    "            done = True\n",
    "        else:\n",
    "            # 5. Append the experience to the memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # 6. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            # 7. Keep a track of rewards, Q-values, loss\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "    if (track_reward == True):\n",
    "        # Track the reward separately for the state [0,0,0]\n",
    "        rewards_init_state.append(score)\n",
    "\n",
    "    # epsilon decay\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "\n",
    "    # every 1o episodes:\n",
    "    if (episode % 10 == 0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon, total_time))\n",
    "    # Save the Q_value of the state, action pair we are tracking\n",
    "    agent.save_tracking_states()\n",
    "    score_tracked.append(score) \n",
    "\n",
    "    if(episode % 500 == 0):\n",
    "        print(\"Saving Model {}\".format(episode))\n",
    "        agent.save(name=\"model_weights.pkl\")\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.states_tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Q-Value convergence for state action pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Q_value for state [0,0,0]  action (0,2)')\n",
    "xaxis = np.asarray(range(0, len(state_tracked_sample)))\n",
    "plt.plot(xaxis,np.asarray(state_tracked_sample))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Reward convergence for the tracked state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Reward for init state [0, 0, 0]')\n",
    "xaxis = np.asarray(range(0, len(score_tracked_sample)))\n",
    "plt.plot(xaxis,np.asarray(score_tracked_sample))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,3000)\n",
    "epsilon = []\n",
    "for i in range(0,3000):\n",
    "    epsilon.append(0 + (1 - 0.00001) * np.exp(-0.0007*i))\n",
    "    z = np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,3000)\n",
    "epsilon = []\n",
    "epsilon_c = 1\n",
    "for i in range(0,3000):\n",
    "    epsilon.append(epsilon_c)\n",
    "    epsilon_c = epsilon_c * 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
