{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "11.0\n",
      "0.0\n",
      "3.0542857142857143\n",
      "7.93705306122449\n"
     ]
    }
   ],
   "source": [
    "print(type(Time_matrix))\n",
    "print(Time_matrix.max())\n",
    "print(Time_matrix.min())\n",
    "print(Time_matrix.mean())\n",
    "print(Time_matrix.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "States_track = collections.defaultdict(dict)\n",
    "Q_dict = collections.defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_state(state):\n",
    "    return '-'.join(str(e) for e in state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise states to be tracked\n",
    "def initialise_tracking_states():\n",
    "    sample_q_values = [([1,2,3],(1,3)),([3,5,6],(4,2)),([4,10,2],(3,4)), ([2,7,0],(0,4))]    #select any 4 Q-values\n",
    "    for q_values in sample_q_values:\n",
    "        state = Q_state(q_values[0])\n",
    "        action = q_values[1]\n",
    "        States_track[state][action] = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tracking_states_old():\n",
    "    \"\"\"Saves the states to dictionary\"\"\"\n",
    "    for state in States_track.keys():\n",
    "        for action in States_track[state].keys():\n",
    "            if state in Q_dict and action in Q_dict[state]:\n",
    "                States_track[state][action].append(Q_dict[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'1-2-3': {(1, 3): []}, '3-5-6': {(4, 2): []}, '4-10-2': {(3, 4): []}, '2-7-0': {(0, 4): []}})\n"
     ]
    }
   ],
   "source": [
    "print(States_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_track = 0\n",
    "num_train = 0\n",
    "num_track_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.06 # 0.06 after fix gave nothing\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_max = 1\n",
    "        #self.epsilon_decay = -0.003 #for 1k\n",
    "        self.epsilon_decay = -0.0007 #for 3k\n",
    "        #self.epsilon_decay = -0.0003 #for 10k\n",
    "        #self.epsilon_decay = -0.0001 #for 20k\n",
    "        #self.epsilon_decay = -0.00003 #for 100k\n",
    "        #self.epsilon_decay = -0.000003 #for 1M\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.batch_size = 32      # for 24*1\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.states_tracked = []\n",
    "        #track_input = np.zeros((1, self.state_size))\n",
    "        #track_input[0] = env.state_encod_arch1([0,0,0]).reshape(1,36)\n",
    "        #self.track_state = track_input[0]\n",
    "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
    "        #print(\"TRACK STATE\")\n",
    "        #print(self.track_state)\n",
    "        self.explore = 0\n",
    "        self.exploit = 0\n",
    "        self.num_track = 0\n",
    "        self.num_train = 0\n",
    "        self.num_train_track = 0\n",
    "        self.hit = False\n",
    "        self.hit_index = 0\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        input_shape = self.state_size\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action_indices, action_list = env.requests(state)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "        #if (0):\n",
    "            self.explore += 1\n",
    "            action_list_index = random.randrange(len(action_indices))\n",
    "            action_space_index = action_indices[action_list_index]\n",
    "        else:\n",
    "            self.exploit += 1\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "            q_value = self.model.predict(state)\n",
    "            q_value =[q_value[0][i] for i in action_indices]\n",
    "            #print(\"EXPLOIT\")\n",
    "            #print(q_value)\n",
    "            #print(q_value[0])\n",
    "            action_list_index = np.argmax(q_value)\n",
    "            action_space_index = action_indices[action_list_index]\n",
    "            #print(q_value[action_index])\n",
    "        return action_space_index, action_list[action_list_index]\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, done):\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        #if (self.hit == True):\n",
    "            #self.hit = False\n",
    "            #q_value = self.model.predict(self.track_state)\n",
    "        #print(q_value.shape)\n",
    "        #print(q_value)\n",
    "        #q_value =[q_value[0][i] for i in action_indices]\n",
    "            #print('NEXT ITERATION AFTER HIT')\n",
    "            #print(q_value)\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            self.num_train += 1\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                #print(state)\n",
    "                #if (i == 0):\n",
    "                    #print(type(update_input[i]))\n",
    "                    #print(update_input[i].shape)\n",
    "                    #print(type(state))\n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                if ((state_space.index(state) == 0) and (action == 2)):\n",
    "                    self.num_train_track += 1\n",
    "                    self.hit = True\n",
    "                    self.hit_index = i\n",
    "                    #print(\"TRAIN\")\n",
    "                    #print(state)\n",
    "                    #print(update_input[i])\n",
    "                    target = self.model.predict(update_input)\n",
    "                    #print('TARGET')\n",
    "                    #print(target[0])\n",
    "                    #print(target[i], reward)\n",
    "                    # target for q-network\n",
    "                    target_qval = self.model.predict(update_output)\n",
    "                    #print('TARGET_qval')\n",
    "                    #print(target_qval[i])            \n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            target = self.model.predict(update_input)\n",
    "            # target for q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "                    \n",
    "            #if (self.hit == True):\n",
    "                #print('AFTER TARGET CALC')\n",
    "                #print(target[self.hit_index])\n",
    "            # model fit\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "    def save_tracking_states(self):\n",
    "        self.num_track += 1\n",
    "            #print(state.shape)\n",
    "            #q_value = self.model.predict(state_input[0])\n",
    "        #print(\"TRACK\")\n",
    "        #print(self.track_state)\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "            #print(q_value.shape)\n",
    "            #print(q_value)\n",
    "        #q_value =[q_value[0][i] for i in action_indices]\n",
    "        self.states_tracked.append(q_value[0][2])\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    while !terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        # 2. Evaluate your reward and next state\n",
    "        # 3. Append the experience to the memory\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 36\n",
    "action_size = 21\n",
    "episode_time = 24*30\n",
    "#n_episodes = 1\n",
    "n_episodes = 3000\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "num_track = 0\n",
    "num_train = 0\n",
    "num_track_train = 0\n",
    "env = CabDriver()\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward 129.0, memory_length 137, epsilon 0.99999 total_time 723.0 step_num 137\n",
      "episode 1, reward -185.0, memory_length 277, epsilon 0.9992902519403939 total_time 730.0 step_num 140\n",
      "episode 2, reward -49.0, memory_length 428, epsilon 0.9985909935330313 total_time 728.0 step_num 151\n",
      "episode 3, reward -331.0, memory_length 564, epsilon 0.9978922244352755 total_time 725.0 step_num 136\n",
      "episode 4, reward -86.0, memory_length 700, epsilon 0.9971939443047295 total_time 721.0 step_num 136\n",
      "episode 5, reward 110.0, memory_length 847, epsilon 0.9964961527992363 total_time 725.0 step_num 147\n",
      "episode 6, reward -313.0, memory_length 978, epsilon 0.9957988495768779 total_time 725.0 step_num 131\n",
      "episode 7, reward -320.0, memory_length 1120, epsilon 0.9951020342959757 total_time 721.0 step_num 142\n",
      "episode 8, reward -286.0, memory_length 1254, epsilon 0.9944057066150902 total_time 725.0 step_num 134\n",
      "episode 9, reward -172.0, memory_length 1389, epsilon 0.9937098661930208 total_time 722.0 step_num 135\n",
      "episode 10, reward 7.0, memory_length 1543, epsilon 0.9930145126888058 total_time 724.0 step_num 154\n",
      "episode 11, reward -122.0, memory_length 1700, epsilon 0.9923196457617219 total_time 730.0 step_num 157\n",
      "episode 12, reward 159.0, memory_length 1834, epsilon 0.9916252650712842 total_time 726.0 step_num 134\n",
      "episode 13, reward -302.0, memory_length 1970, epsilon 0.9909313702772462 total_time 721.0 step_num 136\n",
      "episode 14, reward -95.0, memory_length 2000, epsilon 0.9902379610395996 total_time 721.0 step_num 142\n",
      "episode 15, reward 110.0, memory_length 2000, epsilon 0.9895450370185737 total_time 725.0 step_num 138\n",
      "episode 16, reward -127.0, memory_length 2000, epsilon 0.9888525978746355 total_time 722.0 step_num 136\n",
      "episode 17, reward -199.0, memory_length 2000, epsilon 0.9881606432684903 total_time 722.0 step_num 155\n",
      "episode 18, reward -194.0, memory_length 2000, epsilon 0.98746917286108 total_time 721.0 step_num 141\n",
      "episode 19, reward -65.0, memory_length 2000, epsilon 0.9867781863135842 total_time 724.0 step_num 137\n",
      "episode 20, reward -204.0, memory_length 2000, epsilon 0.9860876832874194 total_time 723.0 step_num 139\n",
      "episode 21, reward -293.0, memory_length 2000, epsilon 0.9853976634442391 total_time 721.0 step_num 118\n",
      "episode 22, reward -205.0, memory_length 2000, epsilon 0.9847081264459336 total_time 725.0 step_num 135\n",
      "episode 23, reward -158.0, memory_length 2000, epsilon 0.9840190719546298 total_time 721.0 step_num 137\n",
      "episode 24, reward -505.0, memory_length 2000, epsilon 0.9833304996326909 total_time 731.0 step_num 151\n",
      "episode 25, reward -38.0, memory_length 2000, epsilon 0.9826424091427166 total_time 724.0 step_num 142\n",
      "episode 26, reward -102.0, memory_length 2000, epsilon 0.9819548001475423 total_time 726.0 step_num 139\n",
      "episode 27, reward 156.0, memory_length 2000, epsilon 0.9812676723102398 total_time 723.0 step_num 148\n",
      "episode 28, reward 13.0, memory_length 2000, epsilon 0.9805810252941164 total_time 721.0 step_num 141\n",
      "episode 29, reward -308.0, memory_length 2000, epsilon 0.979894858762715 total_time 724.0 step_num 133\n",
      "episode 30, reward 58.0, memory_length 2000, epsilon 0.9792091723798139 total_time 721.0 step_num 138\n",
      "episode 31, reward -595.0, memory_length 2000, epsilon 0.978523965809427 total_time 731.0 step_num 155\n",
      "episode 32, reward -5.0, memory_length 2000, epsilon 0.9778392387158028 total_time 721.0 step_num 140\n",
      "episode 33, reward -113.0, memory_length 2000, epsilon 0.9771549907634253 total_time 721.0 step_num 170\n",
      "episode 34, reward -208.0, memory_length 2000, epsilon 0.9764712216170126 total_time 722.0 step_num 131\n",
      "episode 35, reward -191.0, memory_length 2000, epsilon 0.9757879309415182 total_time 724.0 step_num 138\n",
      "episode 36, reward -149.0, memory_length 2000, epsilon 0.9751051184021294 total_time 721.0 step_num 129\n",
      "episode 37, reward 35.0, memory_length 2000, epsilon 0.9744227836642682 total_time 722.0 step_num 120\n",
      "episode 38, reward -32.0, memory_length 2000, epsilon 0.9737409263935904 total_time 721.0 step_num 132\n",
      "episode 39, reward -73.0, memory_length 2000, epsilon 0.9730595462559861 total_time 722.0 step_num 129\n",
      "episode 40, reward 275.0, memory_length 2000, epsilon 0.9723786429175789 total_time 728.0 step_num 131\n",
      "episode 41, reward -203.0, memory_length 2000, epsilon 0.9716982160447263 total_time 721.0 step_num 125\n",
      "episode 42, reward 94.0, memory_length 2000, epsilon 0.9710182653040188 total_time 721.0 step_num 149\n",
      "episode 43, reward -359.0, memory_length 2000, epsilon 0.9703387903622808 total_time 727.0 step_num 144\n",
      "episode 44, reward -29.0, memory_length 2000, epsilon 0.9696597908865696 total_time 733.0 step_num 131\n",
      "episode 45, reward -271.0, memory_length 2000, epsilon 0.9689812665441752 total_time 722.0 step_num 138\n",
      "episode 46, reward 132.0, memory_length 2000, epsilon 0.9683032170026208 total_time 726.0 step_num 145\n",
      "episode 47, reward 1.0, memory_length 2000, epsilon 0.9676256419296622 total_time 727.0 step_num 152\n",
      "episode 48, reward -223.0, memory_length 2000, epsilon 0.9669485409932874 total_time 725.0 step_num 140\n",
      "episode 49, reward -419.0, memory_length 2000, epsilon 0.9662719138617171 total_time 721.0 step_num 129\n",
      "episode 50, reward -90.0, memory_length 2000, epsilon 0.965595760203404 total_time 729.0 step_num 136\n",
      "episode 51, reward -68.0, memory_length 2000, epsilon 0.9649200796870326 total_time 721.0 step_num 141\n",
      "episode 52, reward -22.0, memory_length 2000, epsilon 0.9642448719815195 total_time 728.0 step_num 146\n",
      "episode 53, reward -757.0, memory_length 2000, epsilon 0.9635701367560131 total_time 722.0 step_num 125\n",
      "episode 54, reward -78.0, memory_length 2000, epsilon 0.9628958736798929 total_time 723.0 step_num 135\n",
      "episode 55, reward 78.0, memory_length 2000, epsilon 0.9622220824227702 total_time 726.0 step_num 113\n",
      "episode 56, reward -173.0, memory_length 2000, epsilon 0.9615487626544871 total_time 724.0 step_num 150\n",
      "episode 57, reward -149.0, memory_length 2000, epsilon 0.9608759140451169 total_time 721.0 step_num 135\n",
      "episode 58, reward -164.0, memory_length 2000, epsilon 0.9602035362649637 total_time 724.0 step_num 137\n",
      "episode 59, reward 121.0, memory_length 2000, epsilon 0.9595316289845627 total_time 730.0 step_num 149\n",
      "episode 60, reward -70.0, memory_length 2000, epsilon 0.9588601918746789 total_time 725.0 step_num 134\n",
      "episode 61, reward -68.0, memory_length 2000, epsilon 0.9581892246063084 total_time 730.0 step_num 147\n",
      "episode 62, reward 36.0, memory_length 2000, epsilon 0.9575187268506771 total_time 729.0 step_num 137\n",
      "episode 63, reward -248.0, memory_length 2000, epsilon 0.9568486982792411 total_time 730.0 step_num 139\n",
      "episode 64, reward -150.0, memory_length 2000, epsilon 0.9561791385636864 total_time 723.0 step_num 127\n",
      "episode 65, reward 134.0, memory_length 2000, epsilon 0.9555100473759288 total_time 722.0 step_num 146\n",
      "episode 66, reward -257.0, memory_length 2000, epsilon 0.9548414243881135 total_time 721.0 step_num 139\n",
      "episode 67, reward -30.0, memory_length 2000, epsilon 0.9541732692726153 total_time 726.0 step_num 142\n",
      "episode 68, reward -103.0, memory_length 2000, epsilon 0.9535055817020379 total_time 728.0 step_num 143\n",
      "episode 69, reward -68.0, memory_length 2000, epsilon 0.9528383613492148 total_time 721.0 step_num 155\n",
      "episode 70, reward -120.0, memory_length 2000, epsilon 0.9521716078872079 total_time 735.0 step_num 148\n",
      "episode 71, reward -119.0, memory_length 2000, epsilon 0.9515053209893078 total_time 724.0 step_num 132\n",
      "episode 72, reward 188.0, memory_length 2000, epsilon 0.9508395003290341 total_time 722.0 step_num 125\n",
      "episode 73, reward 30.0, memory_length 2000, epsilon 0.9501741455801346 total_time 723.0 step_num 116\n",
      "episode 74, reward -248.0, memory_length 2000, epsilon 0.9495092564165853 total_time 721.0 step_num 158\n",
      "episode 75, reward -301.0, memory_length 2000, epsilon 0.9488448325125908 total_time 728.0 step_num 147\n",
      "episode 76, reward -84.0, memory_length 2000, epsilon 0.9481808735425831 total_time 726.0 step_num 141\n",
      "episode 77, reward 129.0, memory_length 2000, epsilon 0.9475173791812225 total_time 723.0 step_num 158\n",
      "episode 78, reward -302.0, memory_length 2000, epsilon 0.9468543491033965 total_time 721.0 step_num 140\n",
      "episode 79, reward -437.0, memory_length 2000, epsilon 0.9461917829842207 total_time 721.0 step_num 130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 80, reward -229.0, memory_length 2000, epsilon 0.9455296804990374 total_time 728.0 step_num 133\n",
      "episode 81, reward -54.0, memory_length 2000, epsilon 0.9448680413234165 total_time 729.0 step_num 114\n",
      "episode 82, reward -186.0, memory_length 2000, epsilon 0.9442068651331547 total_time 723.0 step_num 127\n",
      "episode 83, reward -365.0, memory_length 2000, epsilon 0.9435461516042758 total_time 721.0 step_num 140\n",
      "episode 84, reward -118.0, memory_length 2000, epsilon 0.94288590041303 total_time 722.0 step_num 145\n",
      "episode 85, reward -99.0, memory_length 2000, epsilon 0.9422261112358943 total_time 738.0 step_num 170\n",
      "episode 86, reward 238.0, memory_length 2000, epsilon 0.9415667837495718 total_time 721.0 step_num 121\n",
      "episode 87, reward -231.0, memory_length 2000, epsilon 0.9409079176309924 total_time 732.0 step_num 151\n",
      "episode 88, reward -23.0, memory_length 2000, epsilon 0.9402495125573114 total_time 721.0 step_num 136\n",
      "episode 89, reward -345.0, memory_length 2000, epsilon 0.9395915682059103 total_time 726.0 step_num 133\n",
      "episode 90, reward 121.0, memory_length 2000, epsilon 0.9389340842543964 total_time 721.0 step_num 136\n",
      "episode 91, reward -279.0, memory_length 2000, epsilon 0.9382770603806027 total_time 729.0 step_num 145\n",
      "episode 92, reward -172.0, memory_length 2000, epsilon 0.9376204962625873 total_time 722.0 step_num 143\n",
      "episode 93, reward -140.0, memory_length 2000, epsilon 0.9369643915786338 total_time 721.0 step_num 124\n",
      "episode 94, reward -100.0, memory_length 2000, epsilon 0.9363087460072509 total_time 722.0 step_num 132\n",
      "episode 95, reward 83.0, memory_length 2000, epsilon 0.9356535592271722 total_time 725.0 step_num 140\n",
      "episode 96, reward -266.0, memory_length 2000, epsilon 0.9349988309173565 total_time 721.0 step_num 132\n",
      "episode 97, reward -237.0, memory_length 2000, epsilon 0.9343445607569865 total_time 726.0 step_num 141\n",
      "episode 98, reward -352.0, memory_length 2000, epsilon 0.9336907484254698 total_time 722.0 step_num 135\n",
      "episode 99, reward -104.0, memory_length 2000, epsilon 0.9330373936024389 total_time 721.0 step_num 135\n",
      "episode 100, reward -660.0, memory_length 2000, epsilon 0.9323844959677493 total_time 726.0 step_num 144\n",
      "episode 101, reward -13.0, memory_length 2000, epsilon 0.9317320552014813 total_time 728.0 step_num 143\n",
      "episode 102, reward 76.0, memory_length 2000, epsilon 0.9310800709839391 total_time 721.0 step_num 146\n",
      "episode 103, reward -221.0, memory_length 2000, epsilon 0.9304285429956504 total_time 721.0 step_num 154\n",
      "episode 104, reward -25.0, memory_length 2000, epsilon 0.9297774709173662 total_time 725.0 step_num 135\n",
      "episode 105, reward -154.0, memory_length 2000, epsilon 0.9291268544300614 total_time 722.0 step_num 145\n",
      "episode 106, reward -25.0, memory_length 2000, epsilon 0.928476693214934 total_time 725.0 step_num 137\n",
      "episode 107, reward 130.0, memory_length 2000, epsilon 0.9278269869534047 total_time 721.0 step_num 128\n",
      "episode 108, reward -203.0, memory_length 2000, epsilon 0.9271777353271176 total_time 721.0 step_num 158\n",
      "episode 109, reward 94.0, memory_length 2000, epsilon 0.9265289380179395 total_time 730.0 step_num 127\n",
      "episode 110, reward -347.0, memory_length 2000, epsilon 0.9258805947079594 total_time 730.0 step_num 147\n",
      "episode 111, reward -231.0, memory_length 2000, epsilon 0.9252327050794893 total_time 723.0 step_num 139\n",
      "episode 112, reward -29.0, memory_length 2000, epsilon 0.9245852688150632 total_time 724.0 step_num 147\n",
      "episode 113, reward 24.0, memory_length 2000, epsilon 0.9239382855974373 total_time 726.0 step_num 138\n",
      "episode 114, reward -136.0, memory_length 2000, epsilon 0.9232917551095897 total_time 722.0 step_num 131\n",
      "episode 115, reward 151.0, memory_length 2000, epsilon 0.9226456770347208 total_time 724.0 step_num 135\n",
      "episode 116, reward -331.0, memory_length 2000, epsilon 0.922000051056252 total_time 725.0 step_num 136\n",
      "episode 117, reward -338.0, memory_length 2000, epsilon 0.9213548768578267 total_time 721.0 step_num 141\n",
      "episode 118, reward -51.0, memory_length 2000, epsilon 0.9207101541233095 total_time 723.0 step_num 139\n",
      "episode 119, reward -118.0, memory_length 2000, epsilon 0.9200658825367861 total_time 722.0 step_num 120\n",
      "episode 120, reward -259.0, memory_length 2000, epsilon 0.9194220617825638 total_time 725.0 step_num 151\n",
      "episode 121, reward -297.0, memory_length 2000, epsilon 0.91877869154517 total_time 729.0 step_num 129\n",
      "episode 122, reward -277.0, memory_length 2000, epsilon 0.9181357715093534 total_time 725.0 step_num 132\n",
      "episode 123, reward -28.0, memory_length 2000, epsilon 0.9174933013600833 total_time 731.0 step_num 134\n",
      "episode 124, reward -506.0, memory_length 2000, epsilon 0.9168512807825493 total_time 733.0 step_num 122\n",
      "episode 125, reward -317.0, memory_length 2000, epsilon 0.9162097094621612 total_time 733.0 step_num 157\n",
      "episode 126, reward -279.0, memory_length 2000, epsilon 0.915568587084549 total_time 729.0 step_num 133\n",
      "episode 127, reward 56.0, memory_length 2000, epsilon 0.9149279133355628 total_time 725.0 step_num 158\n",
      "episode 128, reward -181.0, memory_length 2000, epsilon 0.9142876879012724 total_time 722.0 step_num 130\n",
      "episode 129, reward -175.0, memory_length 2000, epsilon 0.9136479104679675 total_time 728.0 step_num 135\n",
      "episode 130, reward -42.0, memory_length 2000, epsilon 0.9130085807221568 total_time 723.0 step_num 130\n",
      "episode 131, reward -126.0, memory_length 2000, epsilon 0.912369698350569 total_time 729.0 step_num 126\n",
      "episode 132, reward -108.0, memory_length 2000, epsilon 0.9117312630401518 total_time 729.0 step_num 131\n",
      "episode 133, reward 29.0, memory_length 2000, epsilon 0.9110932744780716 total_time 725.0 step_num 145\n",
      "episode 134, reward -360.0, memory_length 2000, epsilon 0.9104557323517141 total_time 729.0 step_num 123\n",
      "episode 135, reward -97.0, memory_length 2000, epsilon 0.9098186363486838 total_time 725.0 step_num 121\n",
      "episode 136, reward 2.0, memory_length 2000, epsilon 0.9091819861568033 total_time 725.0 step_num 134\n",
      "episode 137, reward -208.0, memory_length 2000, epsilon 0.9085457814641145 total_time 722.0 step_num 117\n",
      "episode 138, reward -68.0, memory_length 2000, epsilon 0.9079100219588766 total_time 721.0 step_num 134\n",
      "episode 139, reward -31.0, memory_length 2000, epsilon 0.9072747073295676 total_time 728.0 step_num 136\n",
      "episode 140, reward -123.0, memory_length 2000, epsilon 0.9066398372648834 total_time 723.0 step_num 130\n",
      "episode 141, reward 62.0, memory_length 2000, epsilon 0.9060054114537377 total_time 722.0 step_num 136\n",
      "episode 142, reward -94.0, memory_length 2000, epsilon 0.9053714295852616 total_time 728.0 step_num 126\n",
      "episode 143, reward -156.0, memory_length 2000, epsilon 0.9047378913488041 total_time 726.0 step_num 163\n",
      "episode 144, reward -5.0, memory_length 2000, epsilon 0.9041047964339316 total_time 730.0 step_num 126\n",
      "episode 145, reward -61.0, memory_length 2000, epsilon 0.9034721445304273 total_time 725.0 step_num 146\n",
      "episode 146, reward -176.0, memory_length 2000, epsilon 0.902839935328292 total_time 721.0 step_num 142\n",
      "episode 147, reward -104.0, memory_length 2000, epsilon 0.902208168517743 total_time 721.0 step_num 143\n",
      "episode 148, reward -162.0, memory_length 2000, epsilon 0.9015768437892147 total_time 729.0 step_num 138\n",
      "episode 149, reward -205.0, memory_length 2000, epsilon 0.9009459608333578 total_time 734.0 step_num 134\n",
      "episode 150, reward -99.0, memory_length 2000, epsilon 0.9003155193410398 total_time 729.0 step_num 120\n",
      "episode 151, reward -14.0, memory_length 2000, epsilon 0.8996855190033443 total_time 721.0 step_num 136\n",
      "episode 152, reward -140.0, memory_length 2000, epsilon 0.8990559595115711 total_time 730.0 step_num 119\n",
      "episode 153, reward -148.0, memory_length 2000, epsilon 0.898426840557236 total_time 728.0 step_num 133\n",
      "episode 154, reward -335.0, memory_length 2000, epsilon 0.8977981618320708 total_time 724.0 step_num 119\n",
      "episode 155, reward -122.0, memory_length 2000, epsilon 0.8971699230280229 total_time 721.0 step_num 128\n",
      "episode 156, reward -518.0, memory_length 2000, epsilon 0.8965421238372551 total_time 721.0 step_num 142\n",
      "episode 157, reward -263.0, memory_length 2000, epsilon 0.895914763952146 total_time 724.0 step_num 134\n",
      "episode 158, reward -190.0, memory_length 2000, epsilon 0.8952878430652892 total_time 722.0 step_num 146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 159, reward -484.0, memory_length 2000, epsilon 0.8946613608694933 total_time 725.0 step_num 136\n",
      "episode 160, reward 94.0, memory_length 2000, epsilon 0.8940353170577823 total_time 730.0 step_num 148\n",
      "episode 161, reward -64.0, memory_length 2000, epsilon 0.8934097113233944 total_time 722.0 step_num 129\n",
      "episode 162, reward 97.0, memory_length 2000, epsilon 0.8927845433597831 total_time 724.0 step_num 150\n",
      "episode 163, reward 82.0, memory_length 2000, epsilon 0.8921598128606157 total_time 736.0 step_num 130\n",
      "episode 164, reward -93.0, memory_length 2000, epsilon 0.8915355195197746 total_time 726.0 step_num 126\n",
      "episode 165, reward -239.0, memory_length 2000, epsilon 0.8909116630313557 total_time 721.0 step_num 122\n",
      "episode 166, reward -108.0, memory_length 2000, epsilon 0.8902882430896697 total_time 729.0 step_num 135\n",
      "episode 167, reward 197.0, memory_length 2000, epsilon 0.8896652593892407 total_time 722.0 step_num 121\n",
      "episode 168, reward 221.0, memory_length 2000, epsilon 0.8890427116248064 total_time 728.0 step_num 120\n",
      "episode 169, reward 125.0, memory_length 2000, epsilon 0.8884205994913187 total_time 722.0 step_num 130\n",
      "episode 170, reward -371.0, memory_length 2000, epsilon 0.8877989226839426 total_time 724.0 step_num 122\n",
      "episode 171, reward -329.0, memory_length 2000, epsilon 0.8871776808980562 total_time 721.0 step_num 160\n",
      "episode 172, reward -226.0, memory_length 2000, epsilon 0.8865568738292513 total_time 731.0 step_num 112\n",
      "episode 173, reward -123.0, memory_length 2000, epsilon 0.8859365011733322 total_time 723.0 step_num 129\n",
      "episode 174, reward -182.0, memory_length 2000, epsilon 0.8853165626263164 total_time 724.0 step_num 147\n",
      "episode 175, reward -155.0, memory_length 2000, epsilon 0.8846970578844342 total_time 724.0 step_num 133\n",
      "episode 176, reward -168.0, memory_length 2000, epsilon 0.8840779866441278 total_time 723.0 step_num 123\n",
      "episode 177, reward -248.0, memory_length 2000, epsilon 0.8834593486020529 total_time 721.0 step_num 130\n",
      "episode 178, reward -290.0, memory_length 2000, epsilon 0.8828411434550761 total_time 733.0 step_num 124\n",
      "episode 179, reward 48.0, memory_length 2000, epsilon 0.8822233709002775 total_time 732.0 step_num 138\n",
      "episode 180, reward 77.0, memory_length 2000, epsilon 0.8816060306349482 total_time 728.0 step_num 136\n",
      "episode 181, reward 16.0, memory_length 2000, epsilon 0.8809891223565917 total_time 733.0 step_num 137\n",
      "episode 182, reward -69.0, memory_length 2000, epsilon 0.8803726457629226 total_time 723.0 step_num 127\n",
      "episode 183, reward -81.0, memory_length 2000, epsilon 0.8797566005518677 total_time 729.0 step_num 124\n",
      "episode 184, reward -302.0, memory_length 2000, epsilon 0.8791409864215646 total_time 730.0 step_num 136\n",
      "episode 185, reward -22.0, memory_length 2000, epsilon 0.8785258030703623 total_time 728.0 step_num 146\n",
      "episode 186, reward -248.0, memory_length 2000, epsilon 0.8779110501968211 total_time 721.0 step_num 140\n",
      "episode 187, reward -351.0, memory_length 2000, epsilon 0.8772967274997123 total_time 729.0 step_num 124\n",
      "episode 188, reward -325.0, memory_length 2000, epsilon 0.8766828346780173 total_time 722.0 step_num 127\n",
      "episode 189, reward -116.0, memory_length 2000, epsilon 0.876069371430929 total_time 727.0 step_num 138\n",
      "episode 190, reward -210.0, memory_length 2000, epsilon 0.87545633745785 total_time 726.0 step_num 128\n",
      "episode 191, reward -408.0, memory_length 2000, epsilon 0.8748437324583941 total_time 735.0 step_num 126\n",
      "episode 192, reward -378.0, memory_length 2000, epsilon 0.8742315561323846 total_time 729.0 step_num 142\n",
      "episode 193, reward -70.0, memory_length 2000, epsilon 0.873619808179855 total_time 725.0 step_num 145\n",
      "episode 194, reward -208.0, memory_length 2000, epsilon 0.8730084883010488 total_time 722.0 step_num 140\n",
      "episode 195, reward -73.0, memory_length 2000, epsilon 0.8723975961964195 total_time 722.0 step_num 145\n",
      "episode 196, reward -199.0, memory_length 2000, epsilon 0.8717871315666298 total_time 722.0 step_num 137\n",
      "episode 197, reward -86.0, memory_length 2000, epsilon 0.871177094112552 total_time 721.0 step_num 131\n",
      "episode 198, reward -81.0, memory_length 2000, epsilon 0.8705674835352676 total_time 729.0 step_num 146\n",
      "episode 199, reward -131.0, memory_length 2000, epsilon 0.8699582995360676 total_time 730.0 step_num 133\n",
      "episode 200, reward 105.0, memory_length 2000, epsilon 0.8693495418164519 total_time 726.0 step_num 142\n",
      "episode 201, reward 53.0, memory_length 2000, epsilon 0.868741210078129 total_time 722.0 step_num 140\n",
      "episode 202, reward 157.0, memory_length 2000, epsilon 0.8681333040230164 total_time 721.0 step_num 131\n",
      "episode 203, reward 206.0, memory_length 2000, epsilon 0.8675258233532401 total_time 722.0 step_num 134\n",
      "episode 204, reward -109.0, memory_length 2000, epsilon 0.8669187677711347 total_time 722.0 step_num 137\n",
      "episode 205, reward -128.0, memory_length 2000, epsilon 0.8663121369792428 total_time 724.0 step_num 138\n",
      "episode 206, reward -154.0, memory_length 2000, epsilon 0.8657059306803154 total_time 722.0 step_num 113\n",
      "episode 207, reward 229.0, memory_length 2000, epsilon 0.8651001485773113 total_time 721.0 step_num 136\n",
      "episode 208, reward 184.0, memory_length 2000, epsilon 0.8644947903733974 total_time 721.0 step_num 125\n",
      "episode 209, reward 34.0, memory_length 2000, epsilon 0.8638898557719481 total_time 724.0 step_num 124\n",
      "episode 210, reward 200.0, memory_length 2000, epsilon 0.8632853444765453 total_time 725.0 step_num 136\n",
      "episode 211, reward -242.0, memory_length 2000, epsilon 0.8626812561909786 total_time 736.0 step_num 120\n",
      "episode 212, reward 31.0, memory_length 2000, epsilon 0.8620775906192447 total_time 721.0 step_num 119\n",
      "episode 213, reward 37.0, memory_length 2000, epsilon 0.8614743474655475 total_time 727.0 step_num 131\n",
      "episode 214, reward -55.0, memory_length 2000, epsilon 0.8608715264342977 total_time 731.0 step_num 136\n",
      "episode 215, reward -121.0, memory_length 2000, epsilon 0.8602691272301131 total_time 728.0 step_num 141\n",
      "episode 216, reward -88.0, memory_length 2000, epsilon 0.8596671495578181 total_time 725.0 step_num 123\n",
      "episode 217, reward -248.0, memory_length 2000, epsilon 0.8590655931224436 total_time 721.0 step_num 135\n",
      "episode 218, reward 111.0, memory_length 2000, epsilon 0.8584644576292269 total_time 723.0 step_num 135\n",
      "episode 219, reward -302.0, memory_length 2000, epsilon 0.8578637427836115 total_time 721.0 step_num 154\n",
      "episode 220, reward -234.0, memory_length 2000, epsilon 0.8572634482912475 total_time 729.0 step_num 128\n",
      "episode 221, reward 24.0, memory_length 2000, epsilon 0.8566635738579901 total_time 726.0 step_num 121\n",
      "episode 222, reward -32.0, memory_length 2000, epsilon 0.856064119189901 total_time 721.0 step_num 135\n",
      "episode 223, reward 124.0, memory_length 2000, epsilon 0.8554650839932475 total_time 724.0 step_num 122\n",
      "episode 224, reward -176.0, memory_length 2000, epsilon 0.8548664679745023 total_time 730.0 step_num 135\n",
      "episode 225, reward -149.0, memory_length 2000, epsilon 0.8542682708403435 total_time 721.0 step_num 113\n",
      "episode 226, reward -73.0, memory_length 2000, epsilon 0.8536704922976543 total_time 722.0 step_num 142\n",
      "episode 227, reward 8.0, memory_length 2000, epsilon 0.8530731320535238 total_time 722.0 step_num 132\n",
      "episode 228, reward -24.0, memory_length 2000, epsilon 0.8524761898152448 total_time 723.0 step_num 124\n",
      "episode 229, reward -1.0, memory_length 2000, epsilon 0.8518796652903159 total_time 722.0 step_num 136\n",
      "episode 230, reward 123.0, memory_length 2000, epsilon 0.8512835581864401 total_time 726.0 step_num 133\n",
      "episode 231, reward -231.0, memory_length 2000, epsilon 0.8506878682115248 total_time 723.0 step_num 124\n",
      "episode 232, reward -14.0, memory_length 2000, epsilon 0.850092595073682 total_time 721.0 step_num 127\n",
      "episode 233, reward -115.0, memory_length 2000, epsilon 0.8494977384812278 total_time 725.0 step_num 133\n",
      "episode 234, reward -71.0, memory_length 2000, epsilon 0.8489032981426824 total_time 727.0 step_num 120\n",
      "episode 235, reward -212.0, memory_length 2000, epsilon 0.84830927376677 total_time 721.0 step_num 133\n",
      "episode 236, reward 94.0, memory_length 2000, epsilon 0.8477156650624188 total_time 721.0 step_num 133\n",
      "episode 237, reward 66.0, memory_length 2000, epsilon 0.8471224717387604 total_time 723.0 step_num 132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 238, reward 121.0, memory_length 2000, epsilon 0.8465296935051303 total_time 721.0 step_num 136\n",
      "episode 239, reward 131.0, memory_length 2000, epsilon 0.8459373300710668 total_time 728.0 step_num 119\n",
      "episode 240, reward -142.0, memory_length 2000, epsilon 0.845345381146312 total_time 734.0 step_num 132\n",
      "episode 241, reward 17.0, memory_length 2000, epsilon 0.8447538464408108 total_time 731.0 step_num 133\n",
      "episode 242, reward -13.0, memory_length 2000, epsilon 0.8441627256647113 total_time 737.0 step_num 131\n",
      "episode 243, reward 173.0, memory_length 2000, epsilon 0.8435720185283643 total_time 725.0 step_num 122\n",
      "episode 244, reward -266.0, memory_length 2000, epsilon 0.8429817247423231 total_time 721.0 step_num 131\n",
      "episode 245, reward -415.0, memory_length 2000, epsilon 0.842391844017344 total_time 722.0 step_num 144\n",
      "episode 246, reward -312.0, memory_length 2000, epsilon 0.8418023760643853 total_time 723.0 step_num 130\n",
      "episode 247, reward -23.0, memory_length 2000, epsilon 0.8412133205946078 total_time 721.0 step_num 136\n",
      "episode 248, reward -22.0, memory_length 2000, epsilon 0.8406246773193743 total_time 728.0 step_num 127\n",
      "episode 249, reward -298.0, memory_length 2000, epsilon 0.8400364459502493 total_time 722.0 step_num 138\n",
      "episode 250, reward 22.0, memory_length 2000, epsilon 0.8394486261989997 total_time 721.0 step_num 134\n",
      "episode 251, reward 49.0, memory_length 2000, epsilon 0.8388612177775938 total_time 721.0 step_num 123\n",
      "episode 252, reward 144.0, memory_length 2000, epsilon 0.8382742203982012 total_time 729.0 step_num 117\n",
      "episode 253, reward 23.0, memory_length 2000, epsilon 0.8376876337731937 total_time 737.0 step_num 115\n",
      "episode 254, reward 37.0, memory_length 2000, epsilon 0.8371014576151432 total_time 727.0 step_num 132\n",
      "episode 255, reward -131.0, memory_length 2000, epsilon 0.8365156916368238 total_time 721.0 step_num 147\n",
      "episode 256, reward -48.0, memory_length 2000, epsilon 0.83593033555121 total_time 726.0 step_num 135\n",
      "episode 257, reward 353.0, memory_length 2000, epsilon 0.8353453890714774 total_time 725.0 step_num 134\n",
      "episode 258, reward -149.0, memory_length 2000, epsilon 0.8347608519110021 total_time 721.0 step_num 141\n",
      "episode 259, reward -154.0, memory_length 2000, epsilon 0.8341767237833609 total_time 722.0 step_num 138\n",
      "episode 260, reward -320.0, memory_length 2000, epsilon 0.8335930044023312 total_time 721.0 step_num 130\n",
      "episode 261, reward 80.0, memory_length 2000, epsilon 0.8330096934818902 total_time 722.0 step_num 136\n",
      "episode 262, reward -154.0, memory_length 2000, epsilon 0.8324267907362157 total_time 722.0 step_num 122\n",
      "episode 263, reward -14.0, memory_length 2000, epsilon 0.8318442958796854 total_time 721.0 step_num 128\n",
      "episode 264, reward 20.0, memory_length 2000, epsilon 0.8312622086268767 total_time 725.0 step_num 116\n",
      "episode 265, reward -218.0, memory_length 2000, epsilon 0.8306805286925668 total_time 724.0 step_num 120\n",
      "episode 266, reward -137.0, memory_length 2000, epsilon 0.8300992557917326 total_time 724.0 step_num 142\n",
      "episode 267, reward 125.0, memory_length 2000, epsilon 0.8295183896395504 total_time 722.0 step_num 131\n",
      "episode 268, reward -329.0, memory_length 2000, epsilon 0.8289379299513956 total_time 730.0 step_num 139\n",
      "episode 269, reward 85.0, memory_length 2000, epsilon 0.8283578764428432 total_time 721.0 step_num 127\n",
      "episode 270, reward 133.0, memory_length 2000, epsilon 0.8277782288296667 total_time 724.0 step_num 129\n",
      "episode 271, reward -77.0, memory_length 2000, epsilon 0.8271989868278389 total_time 739.0 step_num 117\n",
      "episode 272, reward -305.0, memory_length 2000, epsilon 0.8266201501535313 total_time 727.0 step_num 134\n",
      "episode 273, reward 19.0, memory_length 2000, epsilon 0.8260417185231138 total_time 736.0 step_num 123\n",
      "episode 274, reward 70.0, memory_length 2000, epsilon 0.8254636916531549 total_time 724.0 step_num 119\n",
      "episode 275, reward -111.0, memory_length 2000, epsilon 0.8248860692604214 total_time 726.0 step_num 126\n",
      "episode 276, reward -100.0, memory_length 2000, epsilon 0.8243088510618783 total_time 722.0 step_num 124\n",
      "episode 277, reward -109.0, memory_length 2000, epsilon 0.8237320367746888 total_time 722.0 step_num 127\n",
      "episode 278, reward 80.0, memory_length 2000, epsilon 0.8231556261162136 total_time 731.0 step_num 146\n",
      "episode 279, reward -78.0, memory_length 2000, epsilon 0.822579618804012 total_time 732.0 step_num 140\n",
      "episode 280, reward -257.0, memory_length 2000, epsilon 0.8220040145558398 total_time 721.0 step_num 142\n",
      "episode 281, reward -77.0, memory_length 2000, epsilon 0.8214288130896513 total_time 721.0 step_num 121\n",
      "episode 282, reward -374.0, memory_length 2000, epsilon 0.8208540141235976 total_time 721.0 step_num 143\n",
      "episode 283, reward -154.0, memory_length 2000, epsilon 0.8202796173760273 total_time 731.0 step_num 130\n",
      "episode 284, reward -170.0, memory_length 2000, epsilon 0.8197056225654858 total_time 727.0 step_num 125\n",
      "episode 285, reward -203.0, memory_length 2000, epsilon 0.819132029410716 total_time 730.0 step_num 139\n",
      "episode 286, reward 61.0, memory_length 2000, epsilon 0.8185588376306567 total_time 724.0 step_num 122\n",
      "episode 287, reward -87.0, memory_length 2000, epsilon 0.8179860469444444 total_time 723.0 step_num 145\n",
      "episode 288, reward -172.0, memory_length 2000, epsilon 0.8174136570714114 total_time 722.0 step_num 125\n",
      "episode 289, reward 59.0, memory_length 2000, epsilon 0.8168416677310868 total_time 728.0 step_num 128\n",
      "episode 290, reward 236.0, memory_length 2000, epsilon 0.8162700786431957 total_time 725.0 step_num 132\n",
      "episode 291, reward 76.0, memory_length 2000, epsilon 0.8156988895276595 total_time 721.0 step_num 126\n",
      "episode 292, reward 56.0, memory_length 2000, epsilon 0.8151281001045955 total_time 725.0 step_num 144\n",
      "episode 293, reward 193.0, memory_length 2000, epsilon 0.8145577100943168 total_time 721.0 step_num 127\n",
      "episode 294, reward 49.0, memory_length 2000, epsilon 0.8139877192173323 total_time 721.0 step_num 154\n",
      "episode 295, reward -48.0, memory_length 2000, epsilon 0.8134181271943466 total_time 726.0 step_num 131\n",
      "episode 296, reward -428.0, memory_length 2000, epsilon 0.8128489337462593 total_time 721.0 step_num 121\n",
      "episode 297, reward 355.0, memory_length 2000, epsilon 0.812280138594166 total_time 721.0 step_num 120\n",
      "episode 298, reward -68.0, memory_length 2000, epsilon 0.8117117414593569 total_time 721.0 step_num 133\n",
      "episode 299, reward 25.0, memory_length 2000, epsilon 0.8111437420633173 total_time 724.0 step_num 139\n",
      "episode 300, reward 31.0, memory_length 2000, epsilon 0.8105761401277274 total_time 721.0 step_num 125\n",
      "episode 301, reward 50.0, memory_length 2000, epsilon 0.8100089353744625 total_time 728.0 step_num 120\n",
      "episode 302, reward 259.0, memory_length 2000, epsilon 0.8094421275255921 total_time 724.0 step_num 125\n",
      "episode 303, reward -5.0, memory_length 2000, epsilon 0.8088757163033805 total_time 721.0 step_num 120\n",
      "episode 304, reward 86.0, memory_length 2000, epsilon 0.8083097014302859 total_time 728.0 step_num 134\n",
      "episode 305, reward -176.0, memory_length 2000, epsilon 0.8077440826289614 total_time 721.0 step_num 121\n",
      "episode 306, reward 116.0, memory_length 2000, epsilon 0.8071788596222534 total_time 722.0 step_num 143\n",
      "episode 307, reward 57.0, memory_length 2000, epsilon 0.8066140321332027 total_time 723.0 step_num 132\n",
      "episode 308, reward -139.0, memory_length 2000, epsilon 0.806049599885044 total_time 728.0 step_num 136\n",
      "episode 309, reward -68.0, memory_length 2000, epsilon 0.8054855626012054 total_time 721.0 step_num 131\n",
      "episode 310, reward -240.0, memory_length 2000, epsilon 0.8049219200053085 total_time 732.0 step_num 113\n",
      "episode 311, reward -86.0, memory_length 2000, epsilon 0.8043586718211684 total_time 730.0 step_num 132\n",
      "episode 312, reward -113.0, memory_length 2000, epsilon 0.8037958177727937 total_time 721.0 step_num 127\n",
      "episode 313, reward -141.0, memory_length 2000, epsilon 0.803233357584386 total_time 732.0 step_num 116\n",
      "episode 314, reward 102.0, memory_length 2000, epsilon 0.8026712909803393 total_time 723.0 step_num 143\n",
      "episode 315, reward -17.0, memory_length 2000, epsilon 0.8021096176852414 total_time 727.0 step_num 137\n",
      "episode 316, reward -68.0, memory_length 2000, epsilon 0.8015483374238721 total_time 730.0 step_num 142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 317, reward 166.0, memory_length 2000, epsilon 0.8009874499212043 total_time 730.0 step_num 146\n",
      "episode 318, reward -275.0, memory_length 2000, epsilon 0.8004269549024029 total_time 721.0 step_num 125\n",
      "episode 319, reward -122.0, memory_length 2000, epsilon 0.7998668520928254 total_time 721.0 step_num 108\n",
      "episode 320, reward -356.0, memory_length 2000, epsilon 0.7993071412180214 total_time 721.0 step_num 119\n",
      "episode 321, reward -85.0, memory_length 2000, epsilon 0.7987478220037326 total_time 728.0 step_num 133\n",
      "episode 322, reward -3.0, memory_length 2000, epsilon 0.7981888941758928 total_time 726.0 step_num 123\n",
      "episode 323, reward 74.0, memory_length 2000, epsilon 0.7976303574606269 total_time 725.0 step_num 124\n",
      "episode 324, reward -233.0, memory_length 2000, epsilon 0.7970722115842521 total_time 727.0 step_num 127\n",
      "episode 325, reward 3.0, memory_length 2000, epsilon 0.7965144562732769 total_time 723.0 step_num 135\n",
      "episode 326, reward -177.0, memory_length 2000, epsilon 0.7959570912544014 total_time 723.0 step_num 125\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "score_tracked = []\n",
    "num_hits = 0\n",
    "num_track = 0\n",
    "num_train = 0\n",
    "num_track_train = 0\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    # reset at the start of each episode\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    state_size = m+t+d\n",
    "    action_size = len(action_space)\n",
    "    #agent = DQNAgent(state_size, action_size)\n",
    "    #print(state)\n",
    "    #print(state_space)\n",
    "    total_time = 0\n",
    "    step_num = 0\n",
    "    while not done:\n",
    "        step_num = step_num + 1\n",
    "        #print(step_num)\n",
    "        # get action for the current state and take a step in the environment\n",
    "        action_index, action = agent.get_action(state)\n",
    "        if ((state_space.index(state) == 0) and (action_index == 2)):\n",
    "            num_hits = num_hits + 1\n",
    "        #print(state, action)\n",
    "        reward, next_state, step_time = env.step(state, action, Time_matrix)\n",
    "        #print(state, next_state, reward)\n",
    "        total_time += step_time\n",
    "        if (total_time > episode_time):\n",
    "            done = True\n",
    "            #reward = 0\n",
    "        # save the sample <s, a, r, s', done> to the replay memory\n",
    "        agent.append_sample(state, action_index, reward, next_state, done)\n",
    "\n",
    "        # train after each step\n",
    "        agent.train_model()\n",
    "\n",
    "        # add reward to the total score of this episode\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "\n",
    "    # epsilon decay\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "    #agent.epsilon = agent.epsilon * 0.999 # for 2k\n",
    "    #(1 - 0.0001) * np.exp(-0.003*i)\n",
    "    #if agent.epsilon > agent.epsilon_min:\n",
    "    #    agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "    # every episode:\n",
    "    if (episode % 1 == 0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4} step_num {5}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon, total_time, step_num))\n",
    "        agent.save_tracking_states()\n",
    "        score_tracked.append(score) \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "#save_obj(States_track,'States_tracked')   \n",
    "print(elapsed_time)\n",
    "    # every few episodes:\n",
    "        # store q-values of some prespecified state-action pairs\n",
    "        # q_dict = agent.store_q_values()\n",
    "\n",
    "        # save model weights\n",
    "        #agent.save_model_weights(name=\"model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.num_train, agent.num_track, agent.num_train_track, num_hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.explore, agent.exploit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[0][4][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[4][2][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[2][0][2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[0][2][4][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[2][1][6][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[1][0][16][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[0][3][20][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[3][2][20][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time_matrix[2][3][23][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.state_get_loc(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.states_tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "#plt.subplot(221)\n",
    "plt.title('state [2,4,6]  action index 2')\n",
    "xaxis = np.asarray(range(0, len(agent.states_tracked)))\n",
    "plt.plot(xaxis,np.asarray(agent.states_tracked))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "#plt.subplot(221)\n",
    "plt.title('state [2,4,6]  action index 2')\n",
    "xaxis = np.asarray(range(0, len(score_tracked)))\n",
    "plt.plot(xaxis,np.asarray(score_tracked))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,3000)\n",
    "epsilon = []\n",
    "for i in range(0,3000):\n",
    "    epsilon.append(0 + (1 - 0.00001) * np.exp(-0.0007*i))\n",
    "    z = np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,3000)\n",
    "epsilon = []\n",
    "epsilon_c = 1\n",
    "for i in range(0,3000):\n",
    "    epsilon.append(epsilon_c)\n",
    "    epsilon_c = epsilon_c * 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(random.randrange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
