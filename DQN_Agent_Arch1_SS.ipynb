{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "11.0\n",
      "0.0\n",
      "3.0542857142857143\n",
      "7.93705306122449\n"
     ]
    }
   ],
   "source": [
    "print(type(Time_matrix))\n",
    "print(Time_matrix.max())\n",
    "print(Time_matrix.min())\n",
    "print(Time_matrix.mean())\n",
    "print(Time_matrix.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "States_track = collections.defaultdict(dict)\n",
    "Q_dict = collections.defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_state(state):\n",
    "    return '-'.join(str(e) for e in state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise states to be tracked\n",
    "def initialise_tracking_states():\n",
    "    sample_q_values = [([1,2,3],(1,3)),([3,5,6],(4,2)),([4,10,2],(3,4)), ([2,7,0],(0,4))]    #select any 4 Q-values\n",
    "    for q_values in sample_q_values:\n",
    "        state = Q_state(q_values[0])\n",
    "        action = q_values[1]\n",
    "        States_track[state][action] = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_tracking_states():\n",
    "    \"\"\"Saves the states to dictionary\"\"\"\n",
    "    for state in States_track.keys():\n",
    "        for action in States_track[state].keys():\n",
    "            if state in Q_dict and action in Q_dict[state]:\n",
    "                States_track[state][action].append(Q_dict[state][action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialise_tracking_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'1-2-3': {(1, 3): []}, '3-5-6': {(4, 2): []}, '4-10-2': {(3, 4): []}, '2-7-0': {(0, 4): []}})\n"
     ]
    }
   ],
   "source": [
    "print(States_track)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracking the state-action pairs for checking convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining a function to save the Q-dictionary as a pickle file\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.07\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_max = 1\n",
    "        #self.epsilon_decay = 0.0003 #for 10k\n",
    "        self.epsilon_decay = 0.003 #for 1k\n",
    "        self.epsilon_min = 0.01\n",
    "        \n",
    "        self.batch_size = 32        # for 24*1\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        input_shape = self.state_size\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "        action_indices, action_list = env.requests(state)\n",
    "        #print(self.epsilon)\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # explore: choose a random action from all possible actions\n",
    "            # in case of cartpole this will randomly choose an action between 0 and 1\n",
    "            #print('explore')\n",
    "            #if (len(action_indices) == 0):\n",
    "                #print(state)\n",
    "            action_index = random.randrange(len(action_indices))\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            #state = state.reshape(1, self.state_size)\n",
    "            #print(env.state_encod_arch1(state).shape)\n",
    "            #state_input = np.zeros((1, self.state_size))\n",
    "            #state_input[0] = env.state_encod_arch1(state)\n",
    "            #print('Greedy')\n",
    "            #print(state_input[0].shape)\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "            #print(state.shape)\n",
    "            #q_value = self.model.predict(state_input[0])\n",
    "            q_value = self.model.predict(state)\n",
    "            #print(q_value.shape)\n",
    "            #print(q_value)\n",
    "            q_value =[q_value[0][i] for i in action_indices]\n",
    "            action_index = np.argmax(q_value[0])\n",
    "            \n",
    "        #print(action_indices, action_index)\n",
    "        #print(len(action_indices))\n",
    "        #print(self.action_size)\n",
    "        return action_index, action_list[action_index]\n",
    "    # Write your code here:\n",
    "    # get action from model using epsilon-greedy policy\n",
    "    # Decay in Îµ after we generate each sample from the environment\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, done):\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "    # Write your code here:\n",
    "    # save sample <s,a,r,s'> to the replay memory\n",
    "    \n",
    "    \n",
    "    \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        \n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                #print(state)\n",
    "                #if (i == 0):\n",
    "                    #print(type(update_input[i]))\n",
    "                    #print(update_input[i].shape)\n",
    "                    #print(type(state))\n",
    "                update_input[i] = env.state_encod_arch1(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            target = self.model.predict(update_input)\n",
    "            #print(target[0])\n",
    "            # target for q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "\n",
    "            # model fit\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Episodes = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    while !terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        # 2. Evaluate your reward and next state\n",
    "        # 3. Append the experience to the memory\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 36\n",
    "action_size = 21\n",
    "episode_time = 24*30\n",
    "n_episodes = 1000\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "env = CabDriver()\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward -16.0, memory_length 142, epsilon 0.9999 total_time 721.0\n",
      "episode 10, reward -191.0, memory_length 1591, epsilon 0.9703484889951534 total_time 721.0\n",
      "episode 20, reward 88.0, memory_length 2000, epsilon 0.9416703571308903 total_time 722.0\n",
      "episode 30, reward -285.0, memory_length 2000, epsilon 0.9138397921527011 total_time 721.0\n",
      "episode 40, reward -45.0, memory_length 2000, epsilon 0.8868317446734858 total_time 721.0\n",
      "episode 50, reward -20.0, memory_length 2000, epsilon 0.8606219056274154 total_time 723.0\n",
      "episode 60, reward -43.0, memory_length 2000, epsilon 0.8351866843901309 total_time 723.0\n",
      "episode 70, reward 54.0, memory_length 2000, epsilon 0.81050318754559 total_time 724.0\n",
      "episode 80, reward -16.0, memory_length 2000, epsilon 0.7865491982804468 total_time 728.0\n",
      "episode 90, reward -140.0, memory_length 2000, epsilon 0.7633031563874195 total_time 724.0\n",
      "episode 100, reward -199.0, memory_length 2000, epsilon 0.7407441388596497 total_time 731.0\n",
      "episode 110, reward -110.0, memory_length 2000, epsilon 0.718851841058583 total_time 725.0\n",
      "episode 120, reward 312.0, memory_length 2000, epsilon 0.6976065584384239 total_time 727.0\n",
      "episode 130, reward 83.0, memory_length 2000, epsilon 0.6769891688107148 total_time 725.0\n",
      "episode 140, reward -102.0, memory_length 2000, epsilon 0.6569811151330752 total_time 721.0\n",
      "episode 150, reward -68.0, memory_length 2000, epsilon 0.6375643888066111 total_time 724.0\n",
      "episode 160, reward 188.0, memory_length 2000, epsilon 0.6187215134669602 total_time 725.0\n",
      "episode 170, reward -530.0, memory_length 2000, epsilon 0.6004355292543847 total_time 724.0\n",
      "episode 180, reward -313.0, memory_length 2000, epsilon 0.5826899775487523 total_time 726.0\n",
      "episode 190, reward 7.0, memory_length 2000, epsilon 0.5654688861556671 total_time 722.0\n",
      "episode 200, reward -83.0, memory_length 2000, epsilon 0.5487567549304171 total_time 722.0\n",
      "episode 210, reward 26.0, memory_length 2000, epsilon 0.5325385418267965 total_time 722.0\n",
      "episode 220, reward -349.0, memory_length 2000, epsilon 0.51679964935825 total_time 722.0\n",
      "episode 230, reward -214.0, memory_length 2000, epsilon 0.5015259114591488 total_time 724.0\n",
      "episode 240, reward -85.0, memory_length 2000, epsilon 0.4867035807343757 total_time 723.0\n",
      "episode 250, reward -99.0, memory_length 2000, epsilon 0.47231931608574057 total_time 722.0\n",
      "episode 260, reward 15.0, memory_length 2000, epsilon 0.458360170704093 total_time 730.0\n",
      "episode 270, reward 119.0, memory_length 2000, epsilon 0.4448135804163188 total_time 721.0\n",
      "episode 280, reward -376.0, memory_length 2000, epsilon 0.4316673523767368 total_time 722.0\n",
      "episode 290, reward -391.0, memory_length 2000, epsilon 0.41890965409271425 total_time 721.0\n",
      "episode 300, reward -120.0, memory_length 2000, epsilon 0.40652900277462506 total_time 726.0\n",
      "episode 310, reward -407.0, memory_length 2000, epsilon 0.3945142550005639 total_time 722.0\n",
      "episode 320, reward -469.0, memory_length 2000, epsilon 0.38285459668651456 total_time 722.0\n",
      "episode 330, reward 46.0, memory_length 2000, epsilon 0.3715395333529435 total_time 722.0\n",
      "episode 340, reward -39.0, memory_length 2000, epsilon 0.360558880679061 total_time 722.0\n",
      "episode 350, reward 243.0, memory_length 2000, epsilon 0.3499027553362442 total_time 725.0\n",
      "episode 360, reward -214.0, memory_length 2000, epsilon 0.33956156609237464 total_time 725.0\n",
      "episode 370, reward -126.0, memory_length 2000, epsilon 0.32952600517908154 total_time 727.0\n",
      "episode 380, reward 3.0, memory_length 2000, epsilon 0.3197870399141222 total_time 725.0\n",
      "episode 390, reward -74.0, memory_length 2000, epsilon 0.3103359045713585 total_time 721.0\n",
      "episode 400, reward 186.0, memory_length 2000, epsilon 0.3011640924910109 total_time 730.0\n",
      "episode 410, reward 304.0, memory_length 2000, epsilon 0.2922633484230913 total_time 722.0\n",
      "episode 420, reward -251.0, memory_length 2000, epsilon 0.2836256610971204 total_time 724.0\n",
      "episode 430, reward 67.0, memory_length 2000, epsilon 0.27524325601144334 total_time 724.0\n",
      "episode 440, reward -171.0, memory_length 2000, epsilon 0.26710858843565377 total_time 732.0\n",
      "episode 450, reward -51.0, memory_length 2000, epsilon 0.2592143366198269 total_time 729.0\n",
      "episode 460, reward 17.0, memory_length 2000, epsilon 0.25155339520445047 total_time 732.0\n",
      "episode 470, reward -121.0, memory_length 2000, epsilon 0.24411886882512177 total_time 724.0\n",
      "episode 480, reward 262.0, memory_length 2000, epsilon 0.23690406590625354 total_time 726.0\n",
      "episode 490, reward -1.0, memory_length 2000, epsilon 0.22990249263820517 total_time 731.0\n",
      "episode 500, reward -94.0, memory_length 2000, epsilon 0.22310784713241497 total_time 727.0\n",
      "episode 510, reward -84.0, memory_length 2000, epsilon 0.21651401374927548 total_time 722.0\n",
      "episode 520, reward -214.0, memory_length 2000, epsilon 0.21011505759364466 total_time 731.0\n",
      "episode 530, reward -28.0, memory_length 2000, epsilon 0.20390521917304 total_time 732.0\n",
      "episode 540, reward -62.0, memory_length 2000, epsilon 0.1978789092137063 total_time 725.0\n",
      "episode 550, reward -59.0, memory_length 2000, epsilon 0.192030703629892 total_time 723.0\n",
      "episode 560, reward 125.0, memory_length 2000, epsilon 0.18635533864180603 total_time 727.0\n",
      "episode 570, reward -42.0, memory_length 2000, epsilon 0.18084770603786038 total_time 727.0\n",
      "episode 580, reward 396.0, memory_length 2000, epsilon 0.17550284857693516 total_time 731.0\n",
      "episode 590, reward 3.0, memory_length 2000, epsilon 0.1703159555265269 total_time 724.0\n",
      "episode 600, reward -385.0, memory_length 2000, epsilon 0.16528235833276436 total_time 727.0\n",
      "episode 610, reward 25.0, memory_length 2000, epsilon 0.16039752641839522 total_time 724.0\n",
      "episode 620, reward 48.0, memory_length 2000, epsilon 0.1556570631049605 total_time 734.0\n",
      "episode 630, reward -199.0, memory_length 2000, epsilon 0.1510567016554872 total_time 729.0\n",
      "episode 640, reward -44.0, memory_length 2000, epsilon 0.14659230143413712 total_time 725.0\n",
      "episode 650, reward 21.0, memory_length 2000, epsilon 0.14225984417935494 total_time 724.0\n",
      "episode 660, reward 268.0, memory_length 2000, epsilon 0.13805543038716173 total_time 721.0\n",
      "episode 670, reward 25.0, memory_length 2000, epsilon 0.13397527580133806 total_time 733.0\n",
      "episode 680, reward -35.0, memory_length 2000, epsilon 0.13001570800733805 total_time 724.0\n",
      "episode 690, reward -96.0, memory_length 2000, epsilon 0.12617316312686827 total_time 723.0\n",
      "episode 700, reward -2.0, memory_length 2000, epsilon 0.1224441826101566 total_time 725.0\n",
      "episode 710, reward -48.0, memory_length 2000, epsilon 0.11882541012302442 total_time 721.0\n",
      "episode 720, reward -90.0, memory_length 2000, epsilon 0.11531358852595872 total_time 723.0\n",
      "episode 730, reward 45.0, memory_length 2000, epsilon 0.11190555694246715 total_time 728.0\n",
      "episode 740, reward 460.0, memory_length 2000, epsilon 0.10859824791407546 total_time 725.0\n",
      "episode 750, reward 362.0, memory_length 2000, epsilon 0.10538868463940815 total_time 723.0\n",
      "episode 760, reward 211.0, memory_length 2000, epsilon 0.10227397829486588 total_time 723.0\n",
      "episode 770, reward -92.0, memory_length 2000, epsilon 0.0992513254344897 total_time 721.0\n",
      "episode 780, reward 113.0, memory_length 2000, epsilon 0.09631800546666999 total_time 729.0\n",
      "episode 790, reward 48.0, memory_length 2000, epsilon 0.09347137820543067 total_time 726.0\n",
      "episode 800, reward 167.0, memory_length 2000, epsilon 0.09070888149408357 total_time 722.0\n",
      "episode 810, reward 195.0, memory_length 2000, epsilon 0.08802802889911432 total_time 729.0\n",
      "episode 820, reward 308.0, memory_length 2000, epsilon 0.0854264074722245 total_time 726.0\n",
      "episode 830, reward -153.0, memory_length 2000, epsilon 0.08290167557851515 total_time 726.0\n",
      "episode 840, reward -142.0, memory_length 2000, epsilon 0.08045156078885748 total_time 726.0\n",
      "episode 850, reward -300.0, memory_length 2000, epsilon 0.078073857834553 total_time 723.0\n",
      "episode 860, reward 93.0, memory_length 2000, epsilon 0.07576642662244319 total_time 736.0\n",
      "episode 870, reward 50.0, memory_length 2000, epsilon 0.0735271903086808 total_time 722.0\n",
      "episode 880, reward -216.0, memory_length 2000, epsilon 0.07135413342943042 total_time 723.0\n",
      "episode 890, reward -104.0, memory_length 2000, epsilon 0.06924530008681506 total_time 724.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 900, reward 163.0, memory_length 2000, epsilon 0.06719879218847578 total_time 722.0\n",
      "episode 910, reward 79.0, memory_length 2000, epsilon 0.06521276773916071 total_time 732.0\n",
      "episode 920, reward -103.0, memory_length 2000, epsilon 0.06328543918280474 total_time 723.0\n",
      "episode 930, reward 79.0, memory_length 2000, epsilon 0.061415071793608625 total_time 722.0\n",
      "episode 940, reward -134.0, memory_length 2000, epsilon 0.05959998211466847 total_time 721.0\n",
      "episode 950, reward -128.0, memory_length 2000, epsilon 0.05783853644275097 total_time 724.0\n",
      "episode 960, reward -239.0, memory_length 2000, epsilon 0.056129149357850315 total_time 729.0\n",
      "episode 970, reward -229.0, memory_length 2000, epsilon 0.05447028229620294 total_time 728.0\n",
      "episode 980, reward 136.0, memory_length 2000, epsilon 0.052860442165476536 total_time 724.0\n",
      "episode 990, reward -337.0, memory_length 2000, epsilon 0.051298180000885915 total_time 726.0\n",
      "845.7595744132996\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    # reset at the start of each episode\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    state_size = m+t+d\n",
    "    action_size = len(action_space)\n",
    "    #agent = DQNAgent(state_size, action_size)\n",
    "    #print(state)\n",
    "    total_time = 0\n",
    "    step_num = 0\n",
    "    while not done:\n",
    "        step_num = step_num + 1\n",
    "        #print(step_num)\n",
    "        # get action for the current state and take a step in the environment\n",
    "        action_index, action = agent.get_action(state)\n",
    "        reward, next_state, step_time = env.step(state, action, Time_matrix)\n",
    "        total_time += step_time\n",
    "        if (total_time > episode_time):\n",
    "            done = True\n",
    "        # save the sample <s, a, r, s', done> to the replay memory\n",
    "        agent.append_sample(state, action_index, reward, next_state, done)\n",
    "\n",
    "        # train after each step\n",
    "        agent.train_model()\n",
    "\n",
    "        # add reward to the total score of this episode\n",
    "        score += reward\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "\n",
    "    # epsilon decay\n",
    "    agent.epsilon = (1 - 0.0001) * np.exp(agent.epsilon_decay * episode * (-1))\n",
    "    #(1 - 0.0001) * np.exp(-0.003*i)\n",
    "    #if agent.epsilon > agent.epsilon_min:\n",
    "    #    agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "    # every episode:\n",
    "    if (episode % 10 == 0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon, total_time))\n",
    "elapsed_time = time.time() - start_time\n",
    "#save_obj(States_track,'States_tracked')   \n",
    "print(elapsed_time)\n",
    "    # every few episodes:\n",
    "        # store q-values of some prespecified state-action pairs\n",
    "        # q_dict = agent.store_q_values()\n",
    "\n",
    "        # save model weights\n",
    "        #agent.save_model_weights(name=\"model_weights.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.state_get_loc(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf5bf780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,1000)\n",
    "epsilon = []\n",
    "for i in range(0,1000):\n",
    "    epsilon.append(0 + (1 - 0.0001) * np.exp(-0.003*i))\n",
    "    z = np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf6a35c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
