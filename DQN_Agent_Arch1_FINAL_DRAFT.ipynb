{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cab-Driver Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "# for building DQN model\n",
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the environment\n",
    "from Env_new import CabDriver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining Time Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the time matrix provided\n",
    "Time_matrix = np.load(\"TM.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check what the max, min and mean time values are. This will help us in defining the 'next_step' function in the Environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "11.0\n",
      "0.0\n",
      "3.0542857142857143\n",
      "7.93705306122449\n"
     ]
    }
   ],
   "source": [
    "print(type(Time_matrix))\n",
    "print(Time_matrix.max())\n",
    "print(Time_matrix.min())\n",
    "print(Time_matrix.mean())\n",
    "print(Time_matrix.var())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the max time is 11 hours between any 2 points, the next state of the cab driver may increase at most by  1 day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Class\n",
    "\n",
    "If you are using this framework, you need to fill the following to complete the following code block:\n",
    "1. State and Action Size\n",
    "2. Hyperparameters\n",
    "3. Create a neural-network model in function 'build_model()'\n",
    "4. Define epsilon-greedy strategy in function 'get_action()'\n",
    "5. Complete the function 'append_sample()'. This function appends the recent experience tuple <state, action, reward, new-state> to the memory\n",
    "6. Complete the 'train_model()' function with following logic:\n",
    "   - If the memory size is greater than mini-batch size, you randomly sample experiences from memory as per the mini-batch size and do the following:\n",
    "      - Initialise your input and output batch for training the model\n",
    "      - Calculate the target Q value for each sample: reward + gamma*max(Q(s'a,))\n",
    "      - Get Q(s', a) values from the last trained model\n",
    "      - Update the input batch as your encoded state and output batch as your Q-values\n",
    "      - Then fit your DQN model using the updated input and output batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        # Define size of state and action\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # Write here: Specify you hyper parameters for the DQN\n",
    "        self.discount_factor = 0.95\n",
    "        self.learning_rate = 0.01 # 0.06 after fix was better\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_max = 1\n",
    "        #self.epsilon_decay = -0.0005 #for 3k\n",
    "        self.epsilon_decay = -0.00015 #for 20k\n",
    "        self.epsilon_min = 0.00001\n",
    "        \n",
    "        self.batch_size = 32\n",
    "\n",
    "        # create replay memory using deque\n",
    "        self.memory = deque(maxlen=2000)\n",
    "\n",
    "        # Initialize the value of the states tracked\n",
    "        self.states_tracked = []\n",
    "        \n",
    "        # We are going to track state [0,0,0] and action () at index 2 in the action space.\n",
    "        self.track_state = np.array(env.state_encod_arch1([0,0,0])).reshape(1, 36)\n",
    "\n",
    "        # create main model and target model\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    # approximate Q function using Neural Network\n",
    "    def build_model(self):\n",
    "        input_shape = self.state_size\n",
    "        model = Sequential()\n",
    "        # Write your code here: Add layers to your neural nets       \n",
    "        model.add(Dense(32, input_dim=self.state_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(32, activation='relu', kernel_initializer='he_uniform'))\n",
    "        # the output layer: output is of size num_actions\n",
    "        model.add(Dense(self.action_size, activation='relu', kernel_initializer='he_uniform'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        model.summary\n",
    "        return model\n",
    "\n",
    "    def get_action(self, state, possible_actions_index, actions):\n",
    "        \"\"\"\n",
    "        get action in a state according to an epsilon-greedy approach\n",
    "        possible_actions_index, actions are the 'ride requests' that teh driver got.\n",
    "        \"\"\"        \n",
    "        # get action from model using epsilon-greedy policy\n",
    "        # Decay in Îµ after each episode       \n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            # explore: choose a random action from the ride requests\n",
    "            return random.choice(possible_actions_index)\n",
    "        else:\n",
    "            # choose the action with the highest q(s, a)\n",
    "            # the first index corresponds to the batch size, so\n",
    "            # reshape state to (1, state_size) so that the first index corresponds to the batch size\n",
    "            state = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "\n",
    "            # Use the model to predict the Q_values.\n",
    "            q_value = self.model.predict(state)\n",
    "\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "            q_vals_possible = [q_value[0][i] for i in possible_actions_index]\n",
    "\n",
    "            return possible_actions_index[np.argmax(q_vals_possible)]\n",
    "\n",
    "    def append_sample(self, state, action_index, reward, next_state, done):\n",
    "        self.memory.append((state, action_index, reward, next_state, done))\n",
    "        \n",
    "    # pick samples randomly from replay memory (with batch_size) and train the network\n",
    "    def train_model(self):\n",
    "        if len(self.memory) > self.batch_size:\n",
    "            # Sample batch from the memory\n",
    "            mini_batch = random.sample(self.memory, self.batch_size)\n",
    "            # initialise two matrices - update_input and update_output\n",
    "            update_input = np.zeros((self.batch_size, self.state_size))\n",
    "            update_output = np.zeros((self.batch_size, self.state_size))\n",
    "            actions, rewards, done = [], [], []\n",
    "\n",
    "            # populate update_input and update_output and the lists rewards, actions, done\n",
    "            for i in range(self.batch_size):\n",
    "                state, action, reward, next_state, done_boolean = mini_batch[i]\n",
    "                update_input[i] = env.state_encod_arch1(state)     \n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                update_output[i] = env.state_encod_arch1(next_state)\n",
    "                done.append(done_boolean)\n",
    "\n",
    "            # predict the target q-values from states s\n",
    "            target = self.model.predict(update_input)\n",
    "            # target for q-network\n",
    "            target_qval = self.model.predict(update_output)\n",
    "\n",
    "\n",
    "            # update the target values\n",
    "            for i in range(self.batch_size):\n",
    "                if done[i]:\n",
    "                    target[i][actions[i]] = rewards[i]\n",
    "                else: # non-terminal state\n",
    "                    target[i][actions[i]] = rewards[i] + self.discount_factor * np.max(target_qval[i])\n",
    "            # model fit\n",
    "            self.model.fit(update_input, target, batch_size=self.batch_size, epochs=1, verbose=0)\n",
    "            \n",
    "    def save_tracking_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_tracked.append(q_value[0][2])\n",
    "        \n",
    "    def save_test_states(self):\n",
    "        # Use the model to predict the q_value of the state we are tacking.\n",
    "        q_value = self.model.predict(self.track_state)\n",
    "        \n",
    "        # Grab the q_value of the action index that we are tracking.\n",
    "        self.states_test.append(q_value[0][2])\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for episode in range(Episodes):\n",
    "\n",
    "    # Write code here\n",
    "    # Call the environment\n",
    "    # Call all the initialised variables of the environment\n",
    "    \n",
    "\n",
    "    #Call the DQN agent\n",
    "    \n",
    "    \n",
    "    while !terminal_state:\n",
    "        \n",
    "        # Write your code here\n",
    "        # 1. Pick epsilon-greedy action from possible actions for the current state\n",
    "        # 2. Evaluate your reward and next state\n",
    "        # 3. Append the experience to the memory\n",
    "        # 4. Train the model by calling function agent.train_model\n",
    "        # 5. Keep a track of rewards, Q-values, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_time = 24*30 #30 days before which car has to be recharged\n",
    "n_episodes = 20000\n",
    "m = 5\n",
    "t = 24\n",
    "d = 7\n",
    "\n",
    "# Invoke Env class\n",
    "env = CabDriver()\n",
    "action_space, state_space, state = env.reset()\n",
    "\n",
    "# Set up state and action sizes.\n",
    "state_size = m+t+d\n",
    "action_size = len(action_space)\n",
    "\n",
    "# Invoke agent class\n",
    "agent = DQNAgent(action_size=action_size, state_size=state_size)\n",
    "\n",
    "# to store rewards in each episode\n",
    "rewards_per_episode, episodes = [], []\n",
    "# Rewards for state [0,0,0] being tracked.\n",
    "rewards_init_state = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0, reward -101.0, memory_length 159, epsilon 0.99999 total_time 724.0\n",
      "episode 10, reward -296.0, memory_length 1509, epsilon 0.9930145126888058 total_time 729.0\n",
      "episode 20, reward 0.0, memory_length 2000, epsilon 0.9860876832874194 total_time 721.0\n",
      "episode 30, reward -128.0, memory_length 2000, epsilon 0.9792091723798139 total_time 729.0\n",
      "episode 40, reward -308.0, memory_length 2000, epsilon 0.9723786429175789 total_time 725.0\n",
      "episode 50, reward -455.0, memory_length 2000, epsilon 0.965595760203404 total_time 722.0\n",
      "episode 60, reward -177.0, memory_length 2000, epsilon 0.9588601918746789 total_time 722.0\n",
      "episode 70, reward 28.0, memory_length 2000, epsilon 0.9521716078872079 total_time 722.0\n",
      "episode 80, reward -46.0, memory_length 2000, epsilon 0.9455296804990374 total_time 724.0\n",
      "episode 90, reward -44.0, memory_length 2000, epsilon 0.9389340842543964 total_time 725.0\n",
      "episode 100, reward -269.0, memory_length 2000, epsilon 0.9323844959677493 total_time 722.0\n",
      "episode 110, reward -183.0, memory_length 2000, epsilon 0.9258805947079594 total_time 729.0\n",
      "episode 120, reward 1.0, memory_length 2000, epsilon 0.9194220617825638 total_time 730.0\n",
      "episode 130, reward -32.0, memory_length 2000, epsilon 0.9130085807221568 total_time 723.0\n",
      "episode 140, reward -286.0, memory_length 2000, epsilon 0.9066398372648834 total_time 722.0\n",
      "episode 150, reward -378.0, memory_length 2000, epsilon 0.9003155193410398 total_time 726.0\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "score_tracked = []\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    track_reward = False\n",
    "\n",
    "    # reset at the start of each episode\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n",
    "    initial_state = env.state_init\n",
    "    if (initial_state == [0,0,0]):\n",
    "        track_reward = True\n",
    "\n",
    "    total_time = 0  # Total time driver rode in this episode\n",
    "    while not done:\n",
    "        # 1. Get a list of the ride requests driver got.\n",
    "        possible_actions_indices, actions = env.requests(state)\n",
    "        # 2. Pick epsilon-greedy action from possible actions for the current state.\n",
    "        action = agent.get_action(state, possible_actions_indices, actions)\n",
    "\n",
    "        # 3. Evaluate your reward and next state\n",
    "        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
    "\n",
    "        # 4. Total time driver rode in this episode\n",
    "        total_time += step_time\n",
    "        if (total_time > episode_time):\n",
    "            done = True\n",
    "        else:\n",
    "            # 5. Append the experience to the memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # 6. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            # 7. Keep a track of rewards, Q-values, loss\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "    # store total reward obtained in this episode\n",
    "    rewards_per_episode.append(score)\n",
    "    episodes.append(episode)\n",
    "    \n",
    "    if (track_reward == True):\n",
    "        # Track the reward separately for the state [0,0,0]\n",
    "        rewards_init_state.append(score)\n",
    "\n",
    "    # epsilon decay\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "\n",
    "    # every 10 episodes:\n",
    "    if ((episode % 10) == 0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode + 1,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon, total_time))\n",
    "    # Save the Q_value of the state, action pair we are tracking\n",
    "    if ((episode + 1) % 4 == 0):\n",
    "        agent.save_tracking_states()\n",
    "    \n",
    "    # Save the model as pkl file\n",
    "    if((episode + 1) % 1000 == 0):\n",
    "        print(\"Saving Model {}\".format(episode))\n",
    "        agent.save(name=\"model_weights.pkl\")\n",
    "    \n",
    "    # Total rewards per episode\n",
    "    score_tracked.append(score) \n",
    "\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.states_tracked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_tracked_sample = [agent.states_tracked[i] for i in range(len(agent.states_tracked)) if agent.states_tracked[i] < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Q_value for state [0,0,0]  action (0,2)')\n",
    "xaxis = np.asarray(range(0, len(state_tracked_sample)))\n",
    "plt.plot(xaxis,np.asarray(state_tracked_sample))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tracked_sample = [score_tracked[i] for i in range(len(score_tracked)) if (i % 4 == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Rewards per episode')\n",
    "xaxis = np.asarray(range(0, len(score_tracked_sample)))\n",
    "plt.plot(xaxis,np.asarray(score_tracked_sample))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(16,7))\n",
    "plt.title('Reward for init state [0, 0, 0]')\n",
    "xaxis = np.asarray(range(0, len(rewards_init_state)))\n",
    "plt.plot(xaxis,np.asarray(rewards_init_state))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Epsilon-decay sample function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Try building a similar epsilon-decay function for your model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,10000)\n",
    "epsilon = []\n",
    "for i in range(0,10000):\n",
    "    epsilon.append(0 + (1 - 0) * np.exp(-0.0009*i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,3000)\n",
    "epsilon = []\n",
    "for i in range(0,3000):\n",
    "    epsilon.append(0 + (1 - 0.00001) * np.exp(-0.0005*i))\n",
    "    z = np.random.random()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = np.arange(0,3000)\n",
    "epsilon = []\n",
    "epsilon_c = 1\n",
    "for i in range(0,3000):\n",
    "    epsilon.append(epsilon_c)\n",
    "    epsilon_c = epsilon_c * 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0xf9d4b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time, epsilon)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start_time = time.time()\n",
    "score_test = []\n",
    "agent.states_test = []\n",
    "n_episodes = 3000\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    # reset at the start of each episode\n",
    "    env = CabDriver()\n",
    "    action_space, state_space, state = env.reset()\n",
    "    # Save the initial state so that reward can be tracked if initial state is [0,0,0]\n",
    "    initial_state = [0,0,0]\n",
    "\n",
    "\n",
    "    total_time = 0  # Total time driver rode in this episode\n",
    "    while not done:\n",
    "        # 1. Get a list of the ride requests driver got.\n",
    "        possible_actions_indices, actions = env.requests(state)\n",
    "        # 2. Pick epsilon-greedy action from possible actions for the current state.\n",
    "        #action = agent.get_action(state, possible_actions_indices, actions)\n",
    "        state_encode = np.array(env.state_encod_arch1(state)).reshape(1, 36)\n",
    "\n",
    "        # Use the model to predict the Q_values.\n",
    "        q_value = agent.model.predict(state_encode)\n",
    "\n",
    "            # truncate the array to only those actions that are part of the ride  requests.\n",
    "        q_vals_possible = [q_value[0][i] for i in possible_actions_indices]\n",
    "\n",
    "        action = possible_actions_indices[np.argmax(q_vals_possible)]\n",
    "\n",
    "\n",
    "        # 3. Evaluate your reward and next state\n",
    "        reward, next_state, step_time = env.step(state, env.action_space[action], Time_matrix)\n",
    "\n",
    "        # 4. Total time driver rode in this episode\n",
    "        total_time += step_time\n",
    "        if (total_time > episode_time):\n",
    "            done = True\n",
    "        else:\n",
    "            # 5. Append the experience to the memory\n",
    "            agent.append_sample(state, action, reward, next_state, done)\n",
    "            # 6. Train the model by calling function agent.train_model\n",
    "            agent.train_model()\n",
    "            # 7. Keep a track of rewards, Q-values, loss\n",
    "            score += reward\n",
    "            state = next_state\n",
    "\n",
    "    episodes.append(episode)\n",
    "    \n",
    "\n",
    "\n",
    "    # epsilon decay\n",
    "    agent.epsilon = (1 - 0.00001) * np.exp(agent.epsilon_decay * episode)\n",
    "\n",
    "    # every 10 episodes:\n",
    "    if (episode % 10 == 0):\n",
    "        print(\"episode {0}, reward {1}, memory_length {2}, epsilon {3} total_time {4}\".format(episode,\n",
    "                                                                         score,\n",
    "                                                                         len(agent.memory),\n",
    "                                                                         agent.epsilon, total_time))\n",
    "    # Save the Q_value of the state, action pair we are tracking\n",
    "    agent.save_test_states()\n",
    "    \n",
    "    # Total rewards per episode\n",
    "    score_test.append(score) \n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
